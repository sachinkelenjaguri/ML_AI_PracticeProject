{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Batch35_20180118_ANNLab_ClassificationActivity_BackOrders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "    Identify products at risk of backorder before the event occurs so the business has time to react. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Data file contains the historical data for the 8 weeks prior to the week we are trying to predict. The data was taken as weekly snapshots at the start of each week. Columns are defined as follows:\n",
    "\n",
    "    sku - Random ID for the product\n",
    "\n",
    "    national_inv - Current inventory level for the part\n",
    "\n",
    "    lead_time - Transit time for product (if available)\n",
    "\n",
    "    in_transit_qty - Amount of product in transit from source\n",
    "\n",
    "    forecast_3_month - Forecast sales for the next 3 months\n",
    "\n",
    "    forecast_6_month - Forecast sales for the next 6 months\n",
    "\n",
    "    forecast_9_month - Forecast sales for the next 9 months\n",
    "\n",
    "    sales_1_month - Sales quantity for the prior 1 month time period\n",
    "\n",
    "    sales_3_month - Sales quantity for the prior 3 month time period\n",
    "\n",
    "    sales_6_month - Sales quantity for the prior 6 month time period\n",
    "\n",
    "    sales_9_month - Sales quantity for the prior 9 month time period\n",
    "\n",
    "    min_bank - Minimum recommend amount to stock\n",
    "\n",
    "    potential_issue - Source issue for part identified\n",
    "\n",
    "    pieces_past_due - Parts overdue from source\n",
    "\n",
    "    perf_6_month_avg - Source performance for prior 6 month period\n",
    "\n",
    "    perf_12_month_avg - Source performance for prior 12 month period\n",
    "\n",
    "    local_bo_qty - Amount of stock orders overdue\n",
    "\n",
    "    deck_risk - Part risk flag\n",
    "\n",
    "    oe_constraint - Part risk flag\n",
    "\n",
    "    ppap_risk - Part risk flag\n",
    "\n",
    "    stop_auto_buy - Part risk flag\n",
    "\n",
    "    rev_stop - Part risk flag\n",
    "\n",
    "    went_on_backorder - Product actually went on backorder. This is the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "#### Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\skelenja\\\\Desktop\\\\NN\\\\20180120_CSE7321c_Batch35_ANNLabFull'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For setting working directory, if required\n",
    "#os.chdir('path to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BackOrders.csv\",header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the number row and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d6baf1a67bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sku', 'national_inv', 'lead_time', 'in_transit_qty',\n",
       "       'forecast_3_month', 'forecast_6_month', 'forecast_9_month',\n",
       "       'sales_1_month', 'sales_3_month', 'sales_6_month', 'sales_9_month',\n",
       "       'min_bank', 'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
       "       'perf_12_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
       "       'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=61589, step=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the top rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>...</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1888279</td>\n",
       "      <td>117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870557</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1475481</td>\n",
       "      <td>258</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>77</td>\n",
       "      <td>184</td>\n",
       "      <td>46</td>\n",
       "      <td>132</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sku  national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
       "0  1888279           117        NaN               0                 0   \n",
       "1  1870557             7        2.0               0                 0   \n",
       "2  1475481           258       15.0              10                10   \n",
       "\n",
       "   forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
       "0                 0                 0              0              0   \n",
       "1                 0                 0              0              0   \n",
       "2                77               184             46            132   \n",
       "\n",
       "   sales_6_month        ...         pieces_past_due  perf_6_month_avg  \\\n",
       "0             15        ...                       0            -99.00   \n",
       "1              0        ...                       0              0.50   \n",
       "2            256        ...                       0              0.54   \n",
       "\n",
       "  perf_12_month_avg  local_bo_qty  deck_risk  oe_constraint  ppap_risk  \\\n",
       "0            -99.00             0         No             No        Yes   \n",
       "1              0.28             0        Yes             No         No   \n",
       "2              0.70             0         No             No         No   \n",
       "\n",
       "  stop_auto_buy rev_stop went_on_backorder  \n",
       "0           Yes       No                No  \n",
       "1           Yes       No                No  \n",
       "2           Yes       No                No  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows a quick statistic summary of your data using describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>...</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>58186.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48145</td>\n",
       "      <td>61577</td>\n",
       "      <td>53792</td>\n",
       "      <td>59303</td>\n",
       "      <td>61569</td>\n",
       "      <td>50296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.037188e+06</td>\n",
       "      <td>287.721882</td>\n",
       "      <td>7.559619</td>\n",
       "      <td>30.192843</td>\n",
       "      <td>1.692728e+02</td>\n",
       "      <td>3.150413e+02</td>\n",
       "      <td>4.535760e+02</td>\n",
       "      <td>44.742957</td>\n",
       "      <td>150.732631</td>\n",
       "      <td>2.835465e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.605400</td>\n",
       "      <td>-6.264182</td>\n",
       "      <td>-5.863664</td>\n",
       "      <td>1.205361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.564178e+05</td>\n",
       "      <td>4233.906931</td>\n",
       "      <td>6.498952</td>\n",
       "      <td>792.869253</td>\n",
       "      <td>5.286742e+03</td>\n",
       "      <td>9.774362e+03</td>\n",
       "      <td>1.420201e+04</td>\n",
       "      <td>1373.805831</td>\n",
       "      <td>5224.959649</td>\n",
       "      <td>8.872270e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>42.309229</td>\n",
       "      <td>25.537906</td>\n",
       "      <td>24.844514</td>\n",
       "      <td>29.981155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.068628e+06</td>\n",
       "      <td>-2999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.498574e+06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.898033e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.314826e+06</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>3.600000e+01</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.284895e+06</td>\n",
       "      <td>673445.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>170976.000000</td>\n",
       "      <td>1.126656e+06</td>\n",
       "      <td>2.094336e+06</td>\n",
       "      <td>3.062016e+06</td>\n",
       "      <td>295197.000000</td>\n",
       "      <td>934593.000000</td>\n",
       "      <td>1.799099e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>7392.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2999.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sku   national_inv     lead_time  in_transit_qty  \\\n",
       "count   6.158900e+04   61589.000000  58186.000000    61589.000000   \n",
       "unique           NaN            NaN           NaN             NaN   \n",
       "top              NaN            NaN           NaN             NaN   \n",
       "freq             NaN            NaN           NaN             NaN   \n",
       "mean    2.037188e+06     287.721882      7.559619       30.192843   \n",
       "std     6.564178e+05    4233.906931      6.498952      792.869253   \n",
       "min     1.068628e+06   -2999.000000      0.000000        0.000000   \n",
       "25%     1.498574e+06       3.000000      4.000000        0.000000   \n",
       "50%     1.898033e+06      10.000000      8.000000        0.000000   \n",
       "75%     2.314826e+06      57.000000      8.000000        0.000000   \n",
       "max     3.284895e+06  673445.000000     52.000000   170976.000000   \n",
       "\n",
       "        forecast_3_month  forecast_6_month  forecast_9_month  sales_1_month  \\\n",
       "count       6.158900e+04      6.158900e+04      6.158900e+04   61589.000000   \n",
       "unique               NaN               NaN               NaN            NaN   \n",
       "top                  NaN               NaN               NaN            NaN   \n",
       "freq                 NaN               NaN               NaN            NaN   \n",
       "mean        1.692728e+02      3.150413e+02      4.535760e+02      44.742957   \n",
       "std         5.286742e+03      9.774362e+03      1.420201e+04    1373.805831   \n",
       "min         0.000000e+00      0.000000e+00      0.000000e+00       0.000000   \n",
       "25%         0.000000e+00      0.000000e+00      0.000000e+00       0.000000   \n",
       "50%         0.000000e+00      0.000000e+00      0.000000e+00       0.000000   \n",
       "75%         1.200000e+01      2.500000e+01      3.600000e+01       6.000000   \n",
       "max         1.126656e+06      2.094336e+06      3.062016e+06  295197.000000   \n",
       "\n",
       "        sales_3_month  sales_6_month        ...         pieces_past_due  \\\n",
       "count    61589.000000   6.158900e+04        ...            61589.000000   \n",
       "unique            NaN            NaN        ...                     NaN   \n",
       "top               NaN            NaN        ...                     NaN   \n",
       "freq              NaN            NaN        ...                     NaN   \n",
       "mean       150.732631   2.835465e+02        ...                1.605400   \n",
       "std       5224.959649   8.872270e+03        ...               42.309229   \n",
       "min          0.000000   0.000000e+00        ...                0.000000   \n",
       "25%          0.000000   0.000000e+00        ...                0.000000   \n",
       "50%          2.000000   4.000000e+00        ...                0.000000   \n",
       "75%         17.000000   3.400000e+01        ...                0.000000   \n",
       "max     934593.000000   1.799099e+06        ...             7392.000000   \n",
       "\n",
       "        perf_6_month_avg perf_12_month_avg  local_bo_qty  deck_risk  \\\n",
       "count       61589.000000      61589.000000  61589.000000      61589   \n",
       "unique               NaN               NaN           NaN          2   \n",
       "top                  NaN               NaN           NaN         No   \n",
       "freq                 NaN               NaN           NaN      48145   \n",
       "mean           -6.264182         -5.863664      1.205361        NaN   \n",
       "std            25.537906         24.844514     29.981155        NaN   \n",
       "min           -99.000000        -99.000000      0.000000        NaN   \n",
       "25%             0.620000          0.640000      0.000000        NaN   \n",
       "50%             0.820000          0.800000      0.000000        NaN   \n",
       "75%             0.960000          0.950000      0.000000        NaN   \n",
       "max             1.000000          1.000000   2999.000000        NaN   \n",
       "\n",
       "        oe_constraint  ppap_risk stop_auto_buy rev_stop went_on_backorder  \n",
       "count           61589      61589         61589    61589             61589  \n",
       "unique              2          2             2        2                 2  \n",
       "top                No         No           Yes       No                No  \n",
       "freq            61577      53792         59303    61569             50296  \n",
       "mean              NaN        NaN           NaN      NaN               NaN  \n",
       "std               NaN        NaN           NaN      NaN               NaN  \n",
       "min               NaN        NaN           NaN      NaN               NaN  \n",
       "25%               NaN        NaN           NaN      NaN               NaN  \n",
       "50%               NaN        NaN           NaN      NaN               NaN  \n",
       "75%               NaN        NaN           NaN      NaN               NaN  \n",
       "max               NaN        NaN           NaN      NaN               NaN  \n",
       "\n",
       "[11 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display data type of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sku                    int64\n",
       "national_inv           int64\n",
       "lead_time            float64\n",
       "in_transit_qty         int64\n",
       "forecast_3_month       int64\n",
       "forecast_6_month       int64\n",
       "forecast_9_month       int64\n",
       "sales_1_month          int64\n",
       "sales_3_month          int64\n",
       "sales_6_month          int64\n",
       "sales_9_month          int64\n",
       "min_bank               int64\n",
       "potential_issue       object\n",
       "pieces_past_due        int64\n",
       "perf_6_month_avg     float64\n",
       "perf_12_month_avg    float64\n",
       "local_bo_qty           int64\n",
       "deck_risk             object\n",
       "oe_constraint         object\n",
       "ppap_risk             object\n",
       "stop_auto_buy         object\n",
       "rev_stop              object\n",
       "went_on_backorder     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "    sku is Categorical but is interpreted as int64 \n",
    "    potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, and went_on_backorder are also categorical but is interpreted as object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all the attributes to appropriate type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data type conversion\n",
    "\n",
    "    Using astype('category') to convert potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, and went_on_backorder attributes to categorical attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in ['sku', 'potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']:\n",
    "    data[col] = data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display data type of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sku                  category\n",
       "national_inv            int64\n",
       "lead_time             float64\n",
       "in_transit_qty          int64\n",
       "forecast_3_month        int64\n",
       "forecast_6_month        int64\n",
       "forecast_9_month        int64\n",
       "sales_1_month           int64\n",
       "sales_3_month           int64\n",
       "sales_6_month           int64\n",
       "sales_9_month           int64\n",
       "min_bank                int64\n",
       "potential_issue      category\n",
       "pieces_past_due         int64\n",
       "perf_6_month_avg      float64\n",
       "perf_12_month_avg     float64\n",
       "local_bo_qty            int64\n",
       "deck_risk            category\n",
       "oe_constraint        category\n",
       "ppap_risk            category\n",
       "stop_auto_buy        category\n",
       "rev_stop             category\n",
       "went_on_backorder    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete sku attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61589"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(np.unique(data.sku))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop('sku', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data\n",
    "\n",
    "Missing value analysis and dropping the records with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "national_inv            0\n",
       "lead_time            3403\n",
       "in_transit_qty          0\n",
       "forecast_3_month        0\n",
       "forecast_6_month        0\n",
       "forecast_9_month        0\n",
       "sales_1_month           0\n",
       "sales_3_month           0\n",
       "sales_6_month           0\n",
       "sales_9_month           0\n",
       "min_bank                0\n",
       "potential_issue         0\n",
       "pieces_past_due         0\n",
       "perf_6_month_avg        0\n",
       "perf_12_month_avg       0\n",
       "local_bo_qty            0\n",
       "deck_risk               0\n",
       "oe_constraint           0\n",
       "ppap_risk               0\n",
       "stop_auto_buy           0\n",
       "rev_stop                0\n",
       "went_on_backorder       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the number of records before and after missing value records removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61589, 22)\n"
     ]
    }
   ],
   "source": [
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since the number of missing values is about 5%. For initial analysis we ignore all these records\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national_inv         0\n",
      "lead_time            0\n",
      "in_transit_qty       0\n",
      "forecast_3_month     0\n",
      "forecast_6_month     0\n",
      "forecast_9_month     0\n",
      "sales_1_month        0\n",
      "sales_3_month        0\n",
      "sales_6_month        0\n",
      "sales_9_month        0\n",
      "min_bank             0\n",
      "potential_issue      0\n",
      "pieces_past_due      0\n",
      "perf_6_month_avg     0\n",
      "perf_12_month_avg    0\n",
      "local_bo_qty         0\n",
      "deck_risk            0\n",
      "oe_constraint        0\n",
      "ppap_risk            0\n",
      "stop_auto_buy        0\n",
      "rev_stop             0\n",
      "went_on_backorder    0\n",
      "dtype: int64\n",
      "----------------------------------\n",
      "(58186, 22)\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())\n",
    "print(\"----------------------------------\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Categorical to Numeric\n",
    "\n",
    "For some of the models all the independent attribute should be of type numeric and ANN model is one among them.\n",
    "But this data set has some categorial attributes.\n",
    "\n",
    "'pandas.get_dummies' To convert convert categorical variable into dummy/indicator variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
      "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
      "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
      "       'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
      "       'perf_12_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
      "       'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating dummy variables.\n",
    "\n",
    "If we have k levels in a category, then we create k-1 dummy variables as the last one would be redundant. So we use the parameter drop_first in pd.get_dummies function that drops the first level in each of the category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_Attributes = data.select_dtypes(include=['category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(columns=categorical_Attributes, data=data, prefix=categorical_Attributes, prefix_sep=\"_\",drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
      "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
      "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
      "       'pieces_past_due', 'perf_6_month_avg', 'perf_12_month_avg',\n",
      "       'local_bo_qty', 'potential_issue_Yes', 'deck_risk_Yes',\n",
      "       'oe_constraint_Yes', 'ppap_risk_Yes', 'stop_auto_buy_Yes',\n",
      "       'rev_stop_Yes', 'went_on_backorder_Yes'],\n",
      "      dtype='object') (58186, 22)\n"
     ]
    }
   ],
   "source": [
    "print (data.columns, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target attribute distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    47217\n",
       "1    10969\n",
       "Name: went_on_backorder_Yes, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(data['went_on_backorder_Yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split\n",
    "\n",
    "Using sklearn.model_selection.train_test_split\n",
    "\n",
    "    Split arrays or matrices into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing train test split on the data\n",
    "X, y = data.loc[:,data.columns!='went_on_backorder_Yes'].values, data.loc[:,'went_on_backorder_Yes'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify = data['went_on_backorder_Yes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    33052\n",
      "1     7678\n",
      "dtype: int64\n",
      "0    14165\n",
      "1     3291\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#To get the distribution in the target in train and test\n",
    "print(pd.value_counts(y_train))\n",
    "print(pd.value_counts(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building a logistic regression model using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=123, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions on train data\n",
    "train_pred = classifier.predict(X_train)\n",
    "# Predictions on test data\n",
    "test_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix - Train Data: \n",
      " [[32916   136]\n",
      " [ 6912   766]]\n",
      "Confusion Matrix - Test Data: \n",
      " [[14109    56]\n",
      " [ 2962   329]]\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "confusion_matrix_train = confusion_matrix(y_train, train_pred)\n",
    "print(\"Confusion Matrix - Train Data: \\n\", confusion_matrix_train)\n",
    "# Test data\n",
    "confusion_matrix_test= confusion_matrix(y_test, test_pred)\n",
    "print(\"Confusion Matrix - Test Data: \\n\", confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Specificity:  0.995885271693\n",
      "Train Recall:  0.0997655639489\n",
      "Train Precision:  0.849223946785\n",
      "Train Accuracy:  0.826958016204\n"
     ]
    }
   ],
   "source": [
    "# Metrics on train data for logistic regression model\n",
    "#Accuracy\n",
    "accuracy_Train_logReg = (confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Train_logReg = confusion_matrix_train[0,0]/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Train_logReg = confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#precision\n",
    "precision_Train_logReg = confusion_matrix_train[1,1]/(confusion_matrix_train[0,1]+confusion_matrix_train[1,1])\n",
    "\n",
    "print(\"Train Specificity: \",specificity_Train_logReg)\n",
    "print(\"Train Recall: \",recall_Train_logReg)\n",
    "print(\"Train Precision: \",precision_Train_logReg)\n",
    "print(\"Train Accuracy: \",accuracy_Train_logReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Specificity:  0.996046593717\n",
      "Test Recall:  0.0999696140991\n",
      "Test Precision:  0.854545454545\n",
      "Test Accuracy:  0.827108157654\n"
     ]
    }
   ],
   "source": [
    "# Metrics on test data\n",
    "#Accuracy\n",
    "accuracy_Test_logReg = (confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Test_logReg = confusion_matrix_test[0,0]/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Test_logReg = confusion_matrix_test[1,1]/(confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#precision\n",
    "precision_Test_logReg = confusion_matrix_test[1,1]/(confusion_matrix_test[0,1]+confusion_matrix_test[1,1])\n",
    "\n",
    "print(\"Test Specificity: \",specificity_Test_logReg)\n",
    "print(\"Test Recall: \",recall_Test_logReg)\n",
    "print(\"Test Precision: \",precision_Test_logReg)\n",
    "print(\"Test Accuracy: \",accuracy_Test_logReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple ANN model with one hidden layer of 12 nodes   \n",
    "Activation Function: sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann_model = Sequential()\n",
    "\n",
    "# With one hidden layer of 29 nodes\n",
    "ann_model.add(Dense(50, input_dim=21, activation='sigmoid', kernel_initializer='normal'))\n",
    "ann_model.add(Dense(20, activation='sigmoid', kernel_initializer='normal'))\n",
    "ann_model.add(Dense(1, activation='sigmoid', kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/50\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.5226 - acc: 0.8047 - val_loss: 0.4920 - val_acc: 0.8048\n",
      "Epoch 2/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.4773 - acc: 0.8132 - val_loss: 0.4864 - val_acc: 0.8048\n",
      "Epoch 3/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.4719 - acc: 0.8132 - val_loss: 0.4810 - val_acc: 0.8048\n",
      "Epoch 4/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.4659 - acc: 0.8132 - val_loss: 0.4745 - val_acc: 0.8048\n",
      "Epoch 5/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.4595 - acc: 0.8132 - val_loss: 0.4676 - val_acc: 0.8048\n",
      "Epoch 6/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.4524 - acc: 0.8132 - val_loss: 0.4598 - val_acc: 0.8048\n",
      "Epoch 7/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.4441 - acc: 0.8132 - val_loss: 0.4505 - val_acc: 0.8048\n",
      "Epoch 8/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.4346 - acc: 0.8132 - val_loss: 0.4401 - val_acc: 0.8048\n",
      "Epoch 9/50\n",
      "32584/32584 [==============================] - 0s 8us/step - loss: 0.4239 - acc: 0.8132 - val_loss: 0.4284 - val_acc: 0.8048\n",
      "Epoch 10/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.4118 - acc: 0.8132 - val_loss: 0.4150 - val_acc: 0.8048\n",
      "Epoch 11/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.3987 - acc: 0.8132 - val_loss: 0.4011 - val_acc: 0.8048\n",
      "Epoch 12/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.3850 - acc: 0.8132 - val_loss: 0.3867 - val_acc: 0.8048\n",
      "Epoch 13/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.3709 - acc: 0.8132 - val_loss: 0.3723 - val_acc: 0.8048\n",
      "Epoch 14/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.3594 - acc: 0.8132 - val_loss: 0.3623 - val_acc: 0.8048\n",
      "Epoch 15/50\n",
      "32584/32584 [==============================] - 0s 6us/step - loss: 0.3473 - acc: 0.8132 - val_loss: 0.3477 - val_acc: 0.8048\n",
      "Epoch 16/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.3341 - acc: 0.8293 - val_loss: 0.3362 - val_acc: 0.8321\n",
      "Epoch 17/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.3225 - acc: 0.8436 - val_loss: 0.3243 - val_acc: 0.8410\n",
      "Epoch 18/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.3121 - acc: 0.8534 - val_loss: 0.3136 - val_acc: 0.8539\n",
      "Epoch 19/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.3035 - acc: 0.8603 - val_loss: 0.3068 - val_acc: 0.8604\n",
      "Epoch 20/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2956 - acc: 0.8674 - val_loss: 0.2980 - val_acc: 0.8668\n",
      "Epoch 21/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2886 - acc: 0.8726 - val_loss: 0.2910 - val_acc: 0.8700\n",
      "Epoch 22/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2837 - acc: 0.8765 - val_loss: 0.2876 - val_acc: 0.8722\n",
      "Epoch 23/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2786 - acc: 0.8798 - val_loss: 0.2826 - val_acc: 0.8752\n",
      "Epoch 24/50\n",
      "32584/32584 [==============================] - 0s 8us/step - loss: 0.2753 - acc: 0.8821 - val_loss: 0.2779 - val_acc: 0.8781\n",
      "Epoch 25/50\n",
      "32584/32584 [==============================] - 0s 8us/step - loss: 0.2708 - acc: 0.8853 - val_loss: 0.2754 - val_acc: 0.8783\n",
      "Epoch 26/50\n",
      "32584/32584 [==============================] - 0s 8us/step - loss: 0.2678 - acc: 0.8875 - val_loss: 0.2715 - val_acc: 0.8824\n",
      "Epoch 27/50\n",
      "32584/32584 [==============================] - 0s 8us/step - loss: 0.2658 - acc: 0.8883 - val_loss: 0.2745 - val_acc: 0.8806\n",
      "Epoch 28/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2664 - acc: 0.8886 - val_loss: 0.2693 - val_acc: 0.8856\n",
      "Epoch 29/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2628 - acc: 0.8902 - val_loss: 0.2717 - val_acc: 0.8841\n",
      "Epoch 30/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2611 - acc: 0.8918 - val_loss: 0.2623 - val_acc: 0.8883\n",
      "Epoch 31/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2579 - acc: 0.8921 - val_loss: 0.2638 - val_acc: 0.8864\n",
      "Epoch 32/50\n",
      "32584/32584 [==============================] - 0s 8us/step - loss: 0.2568 - acc: 0.8928 - val_loss: 0.2610 - val_acc: 0.8895\n",
      "Epoch 33/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2555 - acc: 0.8935 - val_loss: 0.2609 - val_acc: 0.8915\n",
      "Epoch 34/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2546 - acc: 0.8940 - val_loss: 0.2569 - val_acc: 0.8922\n",
      "Epoch 35/50\n",
      "32584/32584 [==============================] - 0s 8us/step - loss: 0.2522 - acc: 0.8951 - val_loss: 0.2575 - val_acc: 0.8901\n",
      "Epoch 36/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2516 - acc: 0.8956 - val_loss: 0.2565 - val_acc: 0.8930\n",
      "Epoch 37/50\n",
      "32584/32584 [==============================] - 0s 8us/step - loss: 0.2513 - acc: 0.8958 - val_loss: 0.2558 - val_acc: 0.8925\n",
      "Epoch 38/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2506 - acc: 0.8960 - val_loss: 0.2538 - val_acc: 0.8953\n",
      "Epoch 39/50\n",
      "32584/32584 [==============================] - 0s 8us/step - loss: 0.2483 - acc: 0.8972 - val_loss: 0.2554 - val_acc: 0.8910\n",
      "Epoch 40/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2482 - acc: 0.8970 - val_loss: 0.2525 - val_acc: 0.8955\n",
      "Epoch 41/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2482 - acc: 0.8975 - val_loss: 0.2508 - val_acc: 0.8961\n",
      "Epoch 42/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2491 - acc: 0.8972 - val_loss: 0.2536 - val_acc: 0.8945\n",
      "Epoch 43/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2486 - acc: 0.8972 - val_loss: 0.2498 - val_acc: 0.8968\n",
      "Epoch 44/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2478 - acc: 0.8979 - val_loss: 0.2513 - val_acc: 0.8960\n",
      "Epoch 45/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2478 - acc: 0.8991 - val_loss: 0.2504 - val_acc: 0.8975\n",
      "Epoch 46/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2470 - acc: 0.8985 - val_loss: 0.2505 - val_acc: 0.8990\n",
      "Epoch 47/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2477 - acc: 0.8986 - val_loss: 0.2568 - val_acc: 0.8941\n",
      "Epoch 48/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2501 - acc: 0.8975 - val_loss: 0.2536 - val_acc: 0.8950\n",
      "Epoch 49/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2493 - acc: 0.8980 - val_loss: 0.2523 - val_acc: 0.8957\n",
      "Epoch 50/50\n",
      "32584/32584 [==============================] - 0s 7us/step - loss: 0.2480 - acc: 0.8978 - val_loss: 0.2503 - val_acc: 0.8979\n"
     ]
    }
   ],
   "source": [
    "model_history = ann_model.fit(X_train, y_train, epochs=50, batch_size=150, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred = ann_model.predict_classes(X_train)\n",
    "\n",
    "test_pred = ann_model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting evaluation metrics and evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31426  1626]\n",
      " [ 2481  5197]]\n",
      "[[13434   731]\n",
      " [ 1116  2175]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_train = confusion_matrix(y_train, train_pred)\n",
    "confusion_matrix_test = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print(confusion_matrix_train)\n",
    "print(confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy, True Positive Rate and True Negative Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Specificity:  0.950804792448\n",
      "Train Recall:  0.676868976296\n",
      "Train Precision:  0.761688406859\n",
      "Train Accuracy:  0.899165234471\n"
     ]
    }
   ],
   "source": [
    "# Metrics on train data for ann_model 1\n",
    "#Accuracy\n",
    "accuracy_Train_M1 = (confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Train_M1 = confusion_matrix_train[0,0]/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Train_M1 = confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#precision\n",
    "precision_Train_M1 = confusion_matrix_train[1,1]/(confusion_matrix_train[0,1]+confusion_matrix_train[1,1])\n",
    "\n",
    "print(\"Train Specificity: \",specificity_Train_M1)\n",
    "print(\"Train Recall: \",recall_Train_M1)\n",
    "print(\"Train Precision: \",precision_Train_M1)\n",
    "print(\"Train Accuracy: \",accuracy_Train_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Specificity:  0.948393928697\n",
      "Test Recall:  0.660893345488\n",
      "Test Precision:  0.748451479697\n",
      "Test Accuracy:  0.894191109074\n"
     ]
    }
   ],
   "source": [
    "# Metrics on test data\n",
    "#Accuracy\n",
    "accuracy_Test_M1 = (confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Test_M1 = confusion_matrix_test[0,0]/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Test_M1 = confusion_matrix_test[1,1]/(confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#precision\n",
    "precision_Test_M1 = confusion_matrix_test[1,1]/(confusion_matrix_test[0,1]+confusion_matrix_test[1,1])\n",
    "\n",
    "print(\"Test Specificity: \",specificity_Test_M1)\n",
    "print(\"Test Recall: \",recall_Test_M1)\n",
    "print(\"Test Precision: \",precision_Test_M1)\n",
    "print(\"Test Accuracy: \",accuracy_Test_M1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(model_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndNf7HMsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c2447a048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_history.history['acc'])\n",
    "plt.plot(model_history.history['val_acc'])\n",
    "plt.title('Accuracy Plot')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPJ0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c242f3e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Loss Function Plot')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Hidden Layers\n",
    "\n",
    "We have to be careful about overfitting!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/10\n",
      "32584/32584 [==============================] - 5s 147us/step - loss: 0.4738 - acc: 0.8132 - val_loss: 0.4726 - val_acc: 0.8048\n",
      "Epoch 2/10\n",
      "32584/32584 [==============================] - 5s 160us/step - loss: 0.4449 - acc: 0.8132 - val_loss: 0.4374 - val_acc: 0.8048\n",
      "Epoch 3/10\n",
      "32584/32584 [==============================] - 5s 157us/step - loss: 0.4055 - acc: 0.8132 - val_loss: 0.3904 - val_acc: 0.8048\n",
      "Epoch 4/10\n",
      "32584/32584 [==============================] - 5s 157us/step - loss: 0.3611 - acc: 0.8280 - val_loss: 0.3495 - val_acc: 0.8302\n",
      "Epoch 5/10\n",
      "32584/32584 [==============================] - 5s 153us/step - loss: 0.3250 - acc: 0.8543 - val_loss: 0.3145 - val_acc: 0.8582\n",
      "Epoch 6/10\n",
      "32584/32584 [==============================] - 5s 141us/step - loss: 0.2989 - acc: 0.8673 - val_loss: 0.2926 - val_acc: 0.8706\n",
      "Epoch 7/10\n",
      "32584/32584 [==============================] - 5s 157us/step - loss: 0.2810 - acc: 0.8767 - val_loss: 0.2778 - val_acc: 0.8755\n",
      "Epoch 8/10\n",
      "32584/32584 [==============================] - 5s 151us/step - loss: 0.2692 - acc: 0.8832 - val_loss: 0.2712 - val_acc: 0.8786\n",
      "Epoch 9/10\n",
      "32584/32584 [==============================] - 5s 154us/step - loss: 0.2627 - acc: 0.8868 - val_loss: 0.2661 - val_acc: 0.8812\n",
      "Epoch 10/10\n",
      "32584/32584 [==============================] - 5s 149us/step - loss: 0.2581 - acc: 0.8902 - val_loss: 0.2651 - val_acc: 0.8828\n",
      "Confusion matrices:\n",
      "---------------------\n",
      "Confusion matrix - Train:\n",
      "[[31860  1192]\n",
      " [ 3389  4289]]\n",
      "Confusion matrix - Test:\n",
      "[[13652   513]\n",
      " [ 1481  1810]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for new model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.963935616604\n",
      "Train Recall:  0.558609012764\n",
      "Train Precision:  0.782521437694\n",
      "Train Accuracy:  0.887527620918\n",
      "------------------------------------\n",
      "Evaluation metrics on train data for model 1:\n",
      "Train Specificity:  0.916404453588\n",
      "Train Recall:  0.7667361292\n",
      "Train Precision:  0.680578034682\n",
      "Train Accuracy:  0.888190522956\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for new model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.963783974585\n",
      "Test Recall:  0.54998480705\n",
      "Test Precision:  0.779164873009\n",
      "Test Accuracy:  0.885769935839\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for model 1:\n",
      "Test Specificity:  0.91380162372\n",
      "Test Recall:  0.758432087511\n",
      "Test Precision:  0.671509281679\n",
      "Test Accuracy:  0.884509624198\n",
      "------------------------------------\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ann_model_hiddenExp = Sequential()\n",
    "\n",
    "# Adding more hidden layers\n",
    "ann_model_hiddenExp.add(Dense(1000, input_dim=21, activation='sigmoid', kernel_initializer='normal'))\n",
    "ann_model_hiddenExp.add(Dense(500, activation='sigmoid', kernel_initializer='normal'))\n",
    "ann_model_hiddenExp.add(Dense(100, activation='sigmoid', kernel_initializer='normal'))\n",
    "ann_model_hiddenExp.add(Dense(1, activation='sigmoid', kernel_initializer='normal'))\n",
    "\n",
    "ann_model_hiddenExp.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "ann_model_hiddenExp.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Predictions\n",
    "train_pred = ann_model_hiddenExp.predict_classes(X_train)\n",
    "test_pred = ann_model_hiddenExp.predict_classes(X_test)\n",
    "\n",
    "#Evaluation metrics\n",
    "confusion_matrix_train = confusion_matrix(y_train, train_pred)\n",
    "confusion_matrix_test = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print(\"Confusion matrices:\")\n",
    "print(\"---------------------\")\n",
    "print(\"Confusion matrix - Train:\")\n",
    "print(confusion_matrix_train)\n",
    "print(\"Confusion matrix - Test:\")\n",
    "print(confusion_matrix_test)\n",
    "print(\"---------------------\")\n",
    "\n",
    "# Metrics on train data\n",
    "#Accuracy\n",
    "accuracy_Train_hiddenExp = (confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Train_hiddenExp = confusion_matrix_train[0,0]/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Train_hiddenExp = confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#precision\n",
    "precision_Train_hiddenExp = confusion_matrix_train[1,1]/(confusion_matrix_train[0,1]+confusion_matrix_train[1,1])\n",
    "\n",
    "print(\"Evaluation metrics on train data for new model:\")\n",
    "print(\"------------------------------------\")\n",
    "print(\"Train Specificity: \",specificity_Train_hiddenExp)\n",
    "print(\"Train Recall: \",recall_Train_hiddenExp)\n",
    "print(\"Train Precision: \",precision_Train_hiddenExp)\n",
    "print(\"Train Accuracy: \",accuracy_Train_hiddenExp)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"Evaluation metrics on train data for model 1:\")\n",
    "print(\"Train Specificity: \",specificity_Train_M1)\n",
    "print(\"Train Recall: \",recall_Train_M1)\n",
    "print(\"Train Precision: \",precision_Train_M1)\n",
    "print(\"Train Accuracy: \",accuracy_Train_M1)\n",
    "print(\"------------------------------------\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# Metrics on test data\n",
    "#Accuracy\n",
    "accuracy_Test_hiddenExp = (confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Test_hiddenExp = confusion_matrix_test[0,0]/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Test_hiddenExp = confusion_matrix_test[1,1]/(confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#precision\n",
    "precision_Test_hiddenExp = confusion_matrix_test[1,1]/(confusion_matrix_test[0,1]+confusion_matrix_test[1,1])\n",
    "\n",
    "print(\"Evaluation metrics on test data for new model:\")\n",
    "print(\"------------------------------------\")\n",
    "print(\"Test Specificity: \",specificity_Test_hiddenExp)\n",
    "print(\"Test Recall: \",recall_Test_hiddenExp)\n",
    "print(\"Test Precision: \",precision_Test_hiddenExp)\n",
    "print(\"Test Accuracy: \",accuracy_Test_hiddenExp)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"Evaluation metrics on test data for model 1:\")\n",
    "print(\"Test Specificity: \",specificity_Test_M1)\n",
    "print(\"Test Recall: \",recall_Test_M1)\n",
    "print(\"Test Precision: \",precision_Test_M1)\n",
    "print(\"Test Accuracy: \",accuracy_Test_M1)\n",
    "print(\"------------------------------------\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with learning rates\n",
    "\n",
    "Learning rate is an important hyper-parameter for nn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  1e-05\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.6763 - acc: 0.7235 - val_loss: 0.6742 - val_acc: 0.7321\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6716 - acc: 0.7411 - val_loss: 0.6697 - val_acc: 0.7466\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6671 - acc: 0.7611 - val_loss: 0.6654 - val_acc: 0.7715\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 0.6627 - acc: 0.7973 - val_loss: 0.6612 - val_acc: 0.7952\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 0.6584 - acc: 0.8030 - val_loss: 0.6571 - val_acc: 0.7982\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 1s 21us/step - loss: 0.6541 - acc: 0.8046 - val_loss: 0.6530 - val_acc: 0.7992\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6499 - acc: 0.8059 - val_loss: 0.6490 - val_acc: 0.7997\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6458 - acc: 0.8068 - val_loss: 0.6452 - val_acc: 0.8004\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6419 - acc: 0.8074 - val_loss: 0.6414 - val_acc: 0.8008\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.6381 - acc: 0.8077 - val_loss: 0.6379 - val_acc: 0.8009\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 1s 20us/step - loss: 0.6345 - acc: 0.8081 - val_loss: 0.6344 - val_acc: 0.8015\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.6309 - acc: 0.8084 - val_loss: 0.6310 - val_acc: 0.8019\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6274 - acc: 0.8089 - val_loss: 0.6277 - val_acc: 0.8019\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.6240 - acc: 0.8094 - val_loss: 0.6245 - val_acc: 0.8026\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.6207 - acc: 0.8110 - val_loss: 0.6214 - val_acc: 0.8033\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6175 - acc: 0.8121 - val_loss: 0.6184 - val_acc: 0.8042\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.6144 - acc: 0.8129 - val_loss: 0.6154 - val_acc: 0.8047\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.6113 - acc: 0.8131 - val_loss: 0.6125 - val_acc: 0.8048\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6084 - acc: 0.8132 - val_loss: 0.6097 - val_acc: 0.8048\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6055 - acc: 0.8132 - val_loss: 0.6069 - val_acc: 0.8048\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.6027 - acc: 0.8132 - val_loss: 0.6042 - val_acc: 0.8048\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.5999 - acc: 0.8132 - val_loss: 0.6015 - val_acc: 0.8048\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.5972 - acc: 0.8132 - val_loss: 0.5989 - val_acc: 0.8048\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.5944 - acc: 0.8132 - val_loss: 0.5963 - val_acc: 0.8048\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.5917 - acc: 0.8132 - val_loss: 0.5937 - val_acc: 0.8048\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5890 - acc: 0.8132 - val_loss: 0.5911 - val_acc: 0.8048\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5862 - acc: 0.8132 - val_loss: 0.5884 - val_acc: 0.8048\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.5832 - acc: 0.8132 - val_loss: 0.5857 - val_acc: 0.8048\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5807 - acc: 0.8132 - val_loss: 0.5833 - val_acc: 0.8048\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5782 - acc: 0.8132 - val_loss: 0.5810 - val_acc: 0.8048\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5758 - acc: 0.8132 - val_loss: 0.5787 - val_acc: 0.8048\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5735 - acc: 0.8132 - val_loss: 0.5765 - val_acc: 0.8048\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5711 - acc: 0.8132 - val_loss: 0.5743 - val_acc: 0.8048\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5688 - acc: 0.8132 - val_loss: 0.5720 - val_acc: 0.8048\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5663 - acc: 0.8132 - val_loss: 0.5694 - val_acc: 0.8048\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5638 - acc: 0.8132 - val_loss: 0.5674 - val_acc: 0.8048\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5617 - acc: 0.8132 - val_loss: 0.5654 - val_acc: 0.8048\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5597 - acc: 0.8132 - val_loss: 0.5635 - val_acc: 0.8048\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5578 - acc: 0.8132 - val_loss: 0.5617 - val_acc: 0.8048\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5559 - acc: 0.8132 - val_loss: 0.5599 - val_acc: 0.8048\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 1s 20us/step - loss: 0.5540 - acc: 0.8132 - val_loss: 0.5582 - val_acc: 0.8048\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 0.5522 - acc: 0.8132 - val_loss: 0.5564 - val_acc: 0.8048\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.5504 - acc: 0.8132 - val_loss: 0.5547 - val_acc: 0.8048\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 0.5485 - acc: 0.8132 - val_loss: 0.5530 - val_acc: 0.8048\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 0.5468 - acc: 0.8132 - val_loss: 0.5513 - val_acc: 0.8048\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 1s 20us/step - loss: 0.5451 - acc: 0.8132 - val_loss: 0.5498 - val_acc: 0.8048\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 0.5435 - acc: 0.8132 - val_loss: 0.5482 - val_acc: 0.8048\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.5419 - acc: 0.8132 - val_loss: 0.5467 - val_acc: 0.8048\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.5403 - acc: 0.8132 - val_loss: 0.5452 - val_acc: 0.8048\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.5387 - acc: 0.8132 - val_loss: 0.5438 - val_acc: 0.8048\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.5372 - acc: 0.8132 - val_loss: 0.5422 - val_acc: 0.8048\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.5355 - acc: 0.8132 - val_loss: 0.5407 - val_acc: 0.8048\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 1s 15us/step - loss: 0.5340 - acc: 0.8132 - val_loss: 0.5393 - val_acc: 0.8048\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.5326 - acc: 0.8132 - val_loss: 0.5381 - val_acc: 0.8048\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.5313 - acc: 0.8132 - val_loss: 0.5368 - val_acc: 0.8048\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.5300 - acc: 0.8132 - val_loss: 0.5356 - val_acc: 0.8048\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 0.5288 - acc: 0.8132 - val_loss: 0.5345 - val_acc: 0.8048\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5276 - acc: 0.8132 - val_loss: 0.5334 - val_acc: 0.8048\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5264 - acc: 0.8132 - val_loss: 0.5323 - val_acc: 0.8048\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5253 - acc: 0.8132 - val_loss: 0.5312 - val_acc: 0.8048\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5242 - acc: 0.8132 - val_loss: 0.5301 - val_acc: 0.8048\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5231 - acc: 0.8132 - val_loss: 0.5291 - val_acc: 0.8048\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5220 - acc: 0.8132 - val_loss: 0.5281 - val_acc: 0.8048\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5209 - acc: 0.8132 - val_loss: 0.5271 - val_acc: 0.8048\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5199 - acc: 0.8132 - val_loss: 0.5261 - val_acc: 0.8048\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5189 - acc: 0.8132 - val_loss: 0.5252 - val_acc: 0.8048\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5179 - acc: 0.8132 - val_loss: 0.5242 - val_acc: 0.8048\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.5169 - acc: 0.8132 - val_loss: 0.5233 - val_acc: 0.8048\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5159 - acc: 0.8132 - val_loss: 0.5224 - val_acc: 0.8048\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5150 - acc: 0.8132 - val_loss: 0.5215 - val_acc: 0.8048\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5141 - acc: 0.8132 - val_loss: 0.5207 - val_acc: 0.8048\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5131 - acc: 0.8132 - val_loss: 0.5198 - val_acc: 0.8048\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5122 - acc: 0.8132 - val_loss: 0.5189 - val_acc: 0.8048\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5113 - acc: 0.8132 - val_loss: 0.5181 - val_acc: 0.8048\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5105 - acc: 0.8132 - val_loss: 0.5173 - val_acc: 0.8048\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5096 - acc: 0.8132 - val_loss: 0.5165 - val_acc: 0.8048\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5088 - acc: 0.8132 - val_loss: 0.5157 - val_acc: 0.8048\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5079 - acc: 0.8132 - val_loss: 0.5149 - val_acc: 0.8048\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5071 - acc: 0.8132 - val_loss: 0.5141 - val_acc: 0.8048\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5063 - acc: 0.8132 - val_loss: 0.5134 - val_acc: 0.8048\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5055 - acc: 0.8132 - val_loss: 0.5126 - val_acc: 0.8048\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5047 - acc: 0.8132 - val_loss: 0.5119 - val_acc: 0.8048\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5040 - acc: 0.8132 - val_loss: 0.5112 - val_acc: 0.8048\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5032 - acc: 0.8132 - val_loss: 0.5105 - val_acc: 0.8048\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5025 - acc: 0.8132 - val_loss: 0.5098 - val_acc: 0.8048\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5018 - acc: 0.8132 - val_loss: 0.5091 - val_acc: 0.8048\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5011 - acc: 0.8132 - val_loss: 0.5084 - val_acc: 0.8048\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5004 - acc: 0.8132 - val_loss: 0.5078 - val_acc: 0.8048\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4997 - acc: 0.8132 - val_loss: 0.5071 - val_acc: 0.8048\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4990 - acc: 0.8132 - val_loss: 0.5065 - val_acc: 0.8048\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4983 - acc: 0.8132 - val_loss: 0.5059 - val_acc: 0.8048\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4977 - acc: 0.8132 - val_loss: 0.5052 - val_acc: 0.8048\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4970 - acc: 0.8132 - val_loss: 0.5046 - val_acc: 0.8048\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4964 - acc: 0.8132 - val_loss: 0.5040 - val_acc: 0.8048\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4957 - acc: 0.8132 - val_loss: 0.5034 - val_acc: 0.8048\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4951 - acc: 0.8132 - val_loss: 0.5029 - val_acc: 0.8048\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4945 - acc: 0.8132 - val_loss: 0.5023 - val_acc: 0.8048\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4939 - acc: 0.8132 - val_loss: 0.5017 - val_acc: 0.8048\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4933 - acc: 0.8132 - val_loss: 0.5011 - val_acc: 0.8048\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4927 - acc: 0.8132 - val_loss: 0.5006 - val_acc: 0.8048\n",
      "Confusion matrices:\n",
      "---------------------\n",
      "[[33052     0]\n",
      " [ 7678     0]]\n",
      "[[14165     0]\n",
      " [ 3291     0]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  1.0\n",
      "Train Recall:  0.0\n",
      "Train Precision:  nan\n",
      "Train Accuracy:  0.811490301989\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  1.0\n",
      "Test Recall:  0.0\n",
      "Test Precision:  nan\n",
      "Test Accuracy:  0.81146883593\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  0.0001\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogesh/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/yogesh/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 1s 20us/step - loss: 0.6388 - acc: 0.8100 - val_loss: 0.6219 - val_acc: 0.8046\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.6036 - acc: 0.8132 - val_loss: 0.5917 - val_acc: 0.8048\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5755 - acc: 0.8132 - val_loss: 0.5676 - val_acc: 0.8048\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5533 - acc: 0.8132 - val_loss: 0.5484 - val_acc: 0.8048\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5350 - acc: 0.8132 - val_loss: 0.5326 - val_acc: 0.8048\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5204 - acc: 0.8132 - val_loss: 0.5199 - val_acc: 0.8048\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.5080 - acc: 0.8132 - val_loss: 0.5086 - val_acc: 0.8048\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4971 - acc: 0.8132 - val_loss: 0.4987 - val_acc: 0.8048\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4872 - acc: 0.8132 - val_loss: 0.4893 - val_acc: 0.8048\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4782 - acc: 0.8132 - val_loss: 0.4812 - val_acc: 0.8048\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4705 - acc: 0.8132 - val_loss: 0.4743 - val_acc: 0.8048\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4638 - acc: 0.8132 - val_loss: 0.4682 - val_acc: 0.8048\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4579 - acc: 0.8132 - val_loss: 0.4629 - val_acc: 0.8048\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4527 - acc: 0.8132 - val_loss: 0.4582 - val_acc: 0.8048\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4481 - acc: 0.8132 - val_loss: 0.4538 - val_acc: 0.8048\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4437 - acc: 0.8132 - val_loss: 0.4497 - val_acc: 0.8048\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4397 - acc: 0.8132 - val_loss: 0.4461 - val_acc: 0.8048\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4361 - acc: 0.8132 - val_loss: 0.4427 - val_acc: 0.8048\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4327 - acc: 0.8132 - val_loss: 0.4396 - val_acc: 0.8048\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4295 - acc: 0.8132 - val_loss: 0.4364 - val_acc: 0.8048\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4263 - acc: 0.8132 - val_loss: 0.4336 - val_acc: 0.8048\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4234 - acc: 0.8132 - val_loss: 0.4309 - val_acc: 0.8048\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4207 - acc: 0.8132 - val_loss: 0.4283 - val_acc: 0.8048\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4181 - acc: 0.8132 - val_loss: 0.4257 - val_acc: 0.8048\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4155 - acc: 0.8132 - val_loss: 0.4230 - val_acc: 0.8048\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4128 - acc: 0.8132 - val_loss: 0.4202 - val_acc: 0.8048\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4100 - acc: 0.8132 - val_loss: 0.4173 - val_acc: 0.8048\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4070 - acc: 0.8132 - val_loss: 0.4142 - val_acc: 0.8048\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4040 - acc: 0.8132 - val_loss: 0.4114 - val_acc: 0.8048\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4015 - acc: 0.8132 - val_loss: 0.4092 - val_acc: 0.8048\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3994 - acc: 0.8132 - val_loss: 0.4071 - val_acc: 0.8048\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3976 - acc: 0.8132 - val_loss: 0.4053 - val_acc: 0.8048\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3958 - acc: 0.8132 - val_loss: 0.4035 - val_acc: 0.8048\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3940 - acc: 0.8132 - val_loss: 0.4017 - val_acc: 0.8048\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3923 - acc: 0.8132 - val_loss: 0.4000 - val_acc: 0.8048\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3907 - acc: 0.8132 - val_loss: 0.3984 - val_acc: 0.8048\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3890 - acc: 0.8132 - val_loss: 0.3969 - val_acc: 0.8048\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3874 - acc: 0.8132 - val_loss: 0.3953 - val_acc: 0.8048\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3859 - acc: 0.8132 - val_loss: 0.3937 - val_acc: 0.8048\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3844 - acc: 0.8132 - val_loss: 0.3922 - val_acc: 0.8048\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3830 - acc: 0.8132 - val_loss: 0.3908 - val_acc: 0.8048\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3815 - acc: 0.8132 - val_loss: 0.3893 - val_acc: 0.8048\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3801 - acc: 0.8132 - val_loss: 0.3879 - val_acc: 0.8048\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3787 - acc: 0.8132 - val_loss: 0.3864 - val_acc: 0.8048\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3773 - acc: 0.8133 - val_loss: 0.3851 - val_acc: 0.8054\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3758 - acc: 0.8135 - val_loss: 0.3837 - val_acc: 0.8057\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3744 - acc: 0.8137 - val_loss: 0.3822 - val_acc: 0.8057\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3730 - acc: 0.8140 - val_loss: 0.3806 - val_acc: 0.8068\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3714 - acc: 0.8147 - val_loss: 0.3788 - val_acc: 0.8086\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3692 - acc: 0.8200 - val_loss: 0.3760 - val_acc: 0.8182\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3673 - acc: 0.8266 - val_loss: 0.3748 - val_acc: 0.8210\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3661 - acc: 0.8278 - val_loss: 0.3737 - val_acc: 0.8224\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3650 - acc: 0.8288 - val_loss: 0.3726 - val_acc: 0.8231\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3639 - acc: 0.8292 - val_loss: 0.3716 - val_acc: 0.8233\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3628 - acc: 0.8300 - val_loss: 0.3705 - val_acc: 0.8246\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3618 - acc: 0.8303 - val_loss: 0.3695 - val_acc: 0.8249\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3609 - acc: 0.8307 - val_loss: 0.3684 - val_acc: 0.8257\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3598 - acc: 0.8321 - val_loss: 0.3675 - val_acc: 0.8256\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3588 - acc: 0.8325 - val_loss: 0.3665 - val_acc: 0.8258\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3578 - acc: 0.8327 - val_loss: 0.3657 - val_acc: 0.8265\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3569 - acc: 0.8332 - val_loss: 0.3646 - val_acc: 0.8264\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3559 - acc: 0.8331 - val_loss: 0.3636 - val_acc: 0.8270\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3550 - acc: 0.8336 - val_loss: 0.3627 - val_acc: 0.8284\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3541 - acc: 0.8342 - val_loss: 0.3618 - val_acc: 0.8286\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3532 - acc: 0.8345 - val_loss: 0.3609 - val_acc: 0.8284\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3523 - acc: 0.8346 - val_loss: 0.3600 - val_acc: 0.8289\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3514 - acc: 0.8349 - val_loss: 0.3591 - val_acc: 0.8301\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3505 - acc: 0.8353 - val_loss: 0.3583 - val_acc: 0.8300\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3497 - acc: 0.8357 - val_loss: 0.3574 - val_acc: 0.8301\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3488 - acc: 0.8364 - val_loss: 0.3566 - val_acc: 0.8306\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3479 - acc: 0.8366 - val_loss: 0.3557 - val_acc: 0.8305\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3472 - acc: 0.8370 - val_loss: 0.3548 - val_acc: 0.8316\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3463 - acc: 0.8373 - val_loss: 0.3540 - val_acc: 0.8323\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3455 - acc: 0.8374 - val_loss: 0.3532 - val_acc: 0.8329\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3447 - acc: 0.8379 - val_loss: 0.3523 - val_acc: 0.8335\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3439 - acc: 0.8381 - val_loss: 0.3515 - val_acc: 0.8337\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3431 - acc: 0.8385 - val_loss: 0.3507 - val_acc: 0.8343\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3424 - acc: 0.8388 - val_loss: 0.3499 - val_acc: 0.8344\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3416 - acc: 0.8388 - val_loss: 0.3492 - val_acc: 0.8344\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3408 - acc: 0.8391 - val_loss: 0.3484 - val_acc: 0.8353\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3401 - acc: 0.8395 - val_loss: 0.3477 - val_acc: 0.8356\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3393 - acc: 0.8400 - val_loss: 0.3469 - val_acc: 0.8364\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3386 - acc: 0.8402 - val_loss: 0.3462 - val_acc: 0.8366\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3379 - acc: 0.8405 - val_loss: 0.3454 - val_acc: 0.8367\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3371 - acc: 0.8406 - val_loss: 0.3447 - val_acc: 0.8371\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3365 - acc: 0.8411 - val_loss: 0.3440 - val_acc: 0.8376\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3357 - acc: 0.8415 - val_loss: 0.3432 - val_acc: 0.8387\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3351 - acc: 0.8418 - val_loss: 0.3425 - val_acc: 0.8391\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3343 - acc: 0.8420 - val_loss: 0.3419 - val_acc: 0.8384\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3337 - acc: 0.8423 - val_loss: 0.3411 - val_acc: 0.8388\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3330 - acc: 0.8427 - val_loss: 0.3404 - val_acc: 0.8392\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3323 - acc: 0.8430 - val_loss: 0.3398 - val_acc: 0.8393\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3317 - acc: 0.8436 - val_loss: 0.3390 - val_acc: 0.8391\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3311 - acc: 0.8439 - val_loss: 0.3384 - val_acc: 0.8397\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3304 - acc: 0.8443 - val_loss: 0.3377 - val_acc: 0.8399\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3297 - acc: 0.8449 - val_loss: 0.3370 - val_acc: 0.8400\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3291 - acc: 0.8452 - val_loss: 0.3364 - val_acc: 0.8400\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3285 - acc: 0.8454 - val_loss: 0.3357 - val_acc: 0.8404\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3278 - acc: 0.8460 - val_loss: 0.3350 - val_acc: 0.8413\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3272 - acc: 0.8462 - val_loss: 0.3343 - val_acc: 0.8418\n",
      "Confusion matrices:\n",
      "---------------------\n",
      "[[32293   759]\n",
      " [ 5525  2153]]\n",
      "[[13844   321]\n",
      " [ 2401   890]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.977036185405\n",
      "Train Recall:  0.280411565512\n",
      "Train Precision:  0.739354395604\n",
      "Train Accuracy:  0.845715688682\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.977338510413\n",
      "Test Recall:  0.270434518383\n",
      "Test Precision:  0.734929810074\n",
      "Test Accuracy:  0.84406507791\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  0.001\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 1s 21us/step - loss: 0.5130 - acc: 0.8132 - val_loss: 0.4730 - val_acc: 0.8048\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4431 - acc: 0.8132 - val_loss: 0.4338 - val_acc: 0.8048\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.4113 - acc: 0.8132 - val_loss: 0.4091 - val_acc: 0.8048\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3921 - acc: 0.8132 - val_loss: 0.3927 - val_acc: 0.8048\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3778 - acc: 0.8137 - val_loss: 0.3793 - val_acc: 0.8094\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3656 - acc: 0.8205 - val_loss: 0.3678 - val_acc: 0.8191\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3552 - acc: 0.8306 - val_loss: 0.3580 - val_acc: 0.8273\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3462 - acc: 0.8359 - val_loss: 0.3494 - val_acc: 0.8330\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3390 - acc: 0.8386 - val_loss: 0.3434 - val_acc: 0.8355\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3309 - acc: 0.8438 - val_loss: 0.3338 - val_acc: 0.8415\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3244 - acc: 0.8473 - val_loss: 0.3278 - val_acc: 0.8442\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3192 - acc: 0.8511 - val_loss: 0.3227 - val_acc: 0.8481\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3143 - acc: 0.8535 - val_loss: 0.3184 - val_acc: 0.8505\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3096 - acc: 0.8569 - val_loss: 0.3136 - val_acc: 0.8515\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3055 - acc: 0.8592 - val_loss: 0.3094 - val_acc: 0.8560\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3020 - acc: 0.8612 - val_loss: 0.3060 - val_acc: 0.8576\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2991 - acc: 0.8625 - val_loss: 0.3055 - val_acc: 0.8583\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2974 - acc: 0.8642 - val_loss: 0.3027 - val_acc: 0.8604\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2932 - acc: 0.8666 - val_loss: 0.2972 - val_acc: 0.8645\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2910 - acc: 0.8672 - val_loss: 0.2980 - val_acc: 0.8632\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2891 - acc: 0.8701 - val_loss: 0.2924 - val_acc: 0.8671\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2860 - acc: 0.8723 - val_loss: 0.2895 - val_acc: 0.8691\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2839 - acc: 0.8733 - val_loss: 0.2880 - val_acc: 0.8696\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2815 - acc: 0.8750 - val_loss: 0.2853 - val_acc: 0.8717\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2801 - acc: 0.8771 - val_loss: 0.2853 - val_acc: 0.8701\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2784 - acc: 0.8776 - val_loss: 0.2820 - val_acc: 0.8721\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2767 - acc: 0.8791 - val_loss: 0.2805 - val_acc: 0.8731\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2758 - acc: 0.8796 - val_loss: 0.2796 - val_acc: 0.8748\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2740 - acc: 0.8806 - val_loss: 0.2782 - val_acc: 0.8748\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2740 - acc: 0.8812 - val_loss: 0.2803 - val_acc: 0.8740\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2732 - acc: 0.8816 - val_loss: 0.2763 - val_acc: 0.8759\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2704 - acc: 0.8839 - val_loss: 0.2755 - val_acc: 0.8764\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2701 - acc: 0.8835 - val_loss: 0.2761 - val_acc: 0.8756\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2698 - acc: 0.8844 - val_loss: 0.2729 - val_acc: 0.8804\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2686 - acc: 0.8852 - val_loss: 0.2725 - val_acc: 0.8809\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2681 - acc: 0.8861 - val_loss: 0.2723 - val_acc: 0.8820\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2665 - acc: 0.8877 - val_loss: 0.2694 - val_acc: 0.8819\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2657 - acc: 0.8871 - val_loss: 0.2709 - val_acc: 0.8818\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2659 - acc: 0.8879 - val_loss: 0.2682 - val_acc: 0.8839\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2635 - acc: 0.8890 - val_loss: 0.2665 - val_acc: 0.8840\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2624 - acc: 0.8893 - val_loss: 0.2659 - val_acc: 0.8850\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.2626 - acc: 0.8895 - val_loss: 0.2701 - val_acc: 0.8812\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2626 - acc: 0.8899 - val_loss: 0.2648 - val_acc: 0.8868\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2610 - acc: 0.8904 - val_loss: 0.2648 - val_acc: 0.8872\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2601 - acc: 0.8912 - val_loss: 0.2644 - val_acc: 0.8863\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2617 - acc: 0.8902 - val_loss: 0.2656 - val_acc: 0.8866\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.2610 - acc: 0.8911 - val_loss: 0.2643 - val_acc: 0.8862\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2589 - acc: 0.8916 - val_loss: 0.2630 - val_acc: 0.8855\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2580 - acc: 0.8917 - val_loss: 0.2615 - val_acc: 0.8882\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2581 - acc: 0.8921 - val_loss: 0.2619 - val_acc: 0.8869\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2575 - acc: 0.8931 - val_loss: 0.2624 - val_acc: 0.8872\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2564 - acc: 0.8932 - val_loss: 0.2599 - val_acc: 0.8890\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2555 - acc: 0.8933 - val_loss: 0.2603 - val_acc: 0.8889\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2600 - acc: 0.8903 - val_loss: 0.2622 - val_acc: 0.8871\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2582 - acc: 0.8922 - val_loss: 0.2604 - val_acc: 0.8888\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2560 - acc: 0.8941 - val_loss: 0.2593 - val_acc: 0.8889\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2554 - acc: 0.8938 - val_loss: 0.2592 - val_acc: 0.8888\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2549 - acc: 0.8941 - val_loss: 0.2596 - val_acc: 0.8900\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2545 - acc: 0.8945 - val_loss: 0.2579 - val_acc: 0.8895\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2546 - acc: 0.8942 - val_loss: 0.2573 - val_acc: 0.8905\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2534 - acc: 0.8949 - val_loss: 0.2572 - val_acc: 0.8912\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2533 - acc: 0.8948 - val_loss: 0.2577 - val_acc: 0.8900\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2534 - acc: 0.8942 - val_loss: 0.2570 - val_acc: 0.8907\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2527 - acc: 0.8958 - val_loss: 0.2562 - val_acc: 0.8917\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2535 - acc: 0.8941 - val_loss: 0.2568 - val_acc: 0.8920\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2521 - acc: 0.8950 - val_loss: 0.2560 - val_acc: 0.8909\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2520 - acc: 0.8955 - val_loss: 0.2553 - val_acc: 0.8926\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2515 - acc: 0.8955 - val_loss: 0.2545 - val_acc: 0.8921\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2513 - acc: 0.8961 - val_loss: 0.2541 - val_acc: 0.8938\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2514 - acc: 0.8957 - val_loss: 0.2552 - val_acc: 0.8926\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2509 - acc: 0.8956 - val_loss: 0.2534 - val_acc: 0.8932\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2501 - acc: 0.8962 - val_loss: 0.2527 - val_acc: 0.8947\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2499 - acc: 0.8969 - val_loss: 0.2535 - val_acc: 0.8938\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2502 - acc: 0.8964 - val_loss: 0.2531 - val_acc: 0.8936\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2501 - acc: 0.8966 - val_loss: 0.2526 - val_acc: 0.8943\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2498 - acc: 0.8966 - val_loss: 0.2565 - val_acc: 0.8906\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2518 - acc: 0.8957 - val_loss: 0.2548 - val_acc: 0.8928\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2508 - acc: 0.8962 - val_loss: 0.2536 - val_acc: 0.8943\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2499 - acc: 0.8961 - val_loss: 0.2531 - val_acc: 0.8937\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2497 - acc: 0.8964 - val_loss: 0.2529 - val_acc: 0.8934\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2499 - acc: 0.8970 - val_loss: 0.2546 - val_acc: 0.8933\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2505 - acc: 0.8960 - val_loss: 0.2532 - val_acc: 0.8947\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2498 - acc: 0.8960 - val_loss: 0.2521 - val_acc: 0.8953\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2491 - acc: 0.8971 - val_loss: 0.2524 - val_acc: 0.8941\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2491 - acc: 0.8969 - val_loss: 0.2517 - val_acc: 0.8959\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2494 - acc: 0.8971 - val_loss: 0.2526 - val_acc: 0.8959\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2497 - acc: 0.8961 - val_loss: 0.2528 - val_acc: 0.8947\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2494 - acc: 0.8969 - val_loss: 0.2523 - val_acc: 0.8953\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2489 - acc: 0.8972 - val_loss: 0.2505 - val_acc: 0.8959\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2485 - acc: 0.8971 - val_loss: 0.2513 - val_acc: 0.8960\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2498 - acc: 0.8965 - val_loss: 0.2543 - val_acc: 0.8936\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2507 - acc: 0.8971 - val_loss: 0.2540 - val_acc: 0.8936\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2501 - acc: 0.8971 - val_loss: 0.2535 - val_acc: 0.8938\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2497 - acc: 0.8970 - val_loss: 0.2527 - val_acc: 0.8950\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2493 - acc: 0.8973 - val_loss: 0.2527 - val_acc: 0.8945\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2496 - acc: 0.8967 - val_loss: 0.2524 - val_acc: 0.8955\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2488 - acc: 0.8977 - val_loss: 0.2518 - val_acc: 0.8963\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2483 - acc: 0.8979 - val_loss: 0.2512 - val_acc: 0.8958\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2479 - acc: 0.8973 - val_loss: 0.2512 - val_acc: 0.8952\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2475 - acc: 0.8977 - val_loss: 0.2503 - val_acc: 0.8960\n",
      "Confusion matrices:\n",
      "---------------------\n",
      "[[31494  1558]\n",
      " [ 2602  5076]]\n",
      "[[13469   696]\n",
      " [ 1144  2147]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.952862156602\n",
      "Train Recall:  0.661109663975\n",
      "Train Precision:  0.765149231233\n",
      "Train Accuracy:  0.897863982323\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.950864807624\n",
      "Test Recall:  0.652385293224\n",
      "Test Precision:  0.755188181498\n",
      "Test Accuracy:  0.894592117324\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  0.01\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 1s 21us/step - loss: 0.3995 - acc: 0.8184 - val_loss: 0.3415 - val_acc: 0.8400\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3132 - acc: 0.8558 - val_loss: 0.3031 - val_acc: 0.8626\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2865 - acc: 0.8729 - val_loss: 0.2897 - val_acc: 0.8695\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2807 - acc: 0.8792 - val_loss: 0.2761 - val_acc: 0.8807\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2735 - acc: 0.8837 - val_loss: 0.2786 - val_acc: 0.8808\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2670 - acc: 0.8884 - val_loss: 0.2633 - val_acc: 0.8877\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2622 - acc: 0.8901 - val_loss: 0.2664 - val_acc: 0.8871\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2679 - acc: 0.8885 - val_loss: 0.2747 - val_acc: 0.8850\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2599 - acc: 0.8938 - val_loss: 0.2622 - val_acc: 0.8876\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2627 - acc: 0.8912 - val_loss: 0.2632 - val_acc: 0.8925\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2599 - acc: 0.8932 - val_loss: 0.2684 - val_acc: 0.8911\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2617 - acc: 0.8927 - val_loss: 0.2627 - val_acc: 0.8923\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2566 - acc: 0.8953 - val_loss: 0.2728 - val_acc: 0.8842\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2656 - acc: 0.8906 - val_loss: 0.2719 - val_acc: 0.8876\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2588 - acc: 0.8923 - val_loss: 0.2601 - val_acc: 0.8930\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2614 - acc: 0.8932 - val_loss: 0.2736 - val_acc: 0.8931\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2658 - acc: 0.8932 - val_loss: 0.2659 - val_acc: 0.8920\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2565 - acc: 0.8947 - val_loss: 0.2575 - val_acc: 0.8925\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2536 - acc: 0.8946 - val_loss: 0.2534 - val_acc: 0.8930\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2496 - acc: 0.8970 - val_loss: 0.2585 - val_acc: 0.8931\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2524 - acc: 0.8963 - val_loss: 0.2918 - val_acc: 0.8598\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2552 - acc: 0.8948 - val_loss: 0.2607 - val_acc: 0.8930\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2550 - acc: 0.8943 - val_loss: 0.2533 - val_acc: 0.8922\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2528 - acc: 0.8938 - val_loss: 0.2550 - val_acc: 0.8909\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2507 - acc: 0.8955 - val_loss: 0.2621 - val_acc: 0.8891\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2525 - acc: 0.8949 - val_loss: 0.2509 - val_acc: 0.8957\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2483 - acc: 0.8957 - val_loss: 0.2525 - val_acc: 0.8927\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2526 - acc: 0.8958 - val_loss: 0.2602 - val_acc: 0.8878\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2578 - acc: 0.8933 - val_loss: 0.2556 - val_acc: 0.8937\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2519 - acc: 0.8954 - val_loss: 0.2613 - val_acc: 0.8895\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2625 - acc: 0.8911 - val_loss: 0.2690 - val_acc: 0.8887\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2536 - acc: 0.8949 - val_loss: 0.2596 - val_acc: 0.8872\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2516 - acc: 0.8955 - val_loss: 0.2592 - val_acc: 0.8910\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2578 - acc: 0.8930 - val_loss: 0.2543 - val_acc: 0.8912\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2528 - acc: 0.8946 - val_loss: 0.2540 - val_acc: 0.8896\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2552 - acc: 0.8952 - val_loss: 0.2635 - val_acc: 0.8867\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2576 - acc: 0.8947 - val_loss: 0.2596 - val_acc: 0.8853\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2540 - acc: 0.8952 - val_loss: 0.2621 - val_acc: 0.8888\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2520 - acc: 0.8960 - val_loss: 0.2604 - val_acc: 0.8869\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.2582 - acc: 0.8930 - val_loss: 0.2528 - val_acc: 0.8915\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2513 - acc: 0.8952 - val_loss: 0.2580 - val_acc: 0.8901\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2501 - acc: 0.8971 - val_loss: 0.2616 - val_acc: 0.8862\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2542 - acc: 0.8949 - val_loss: 0.2550 - val_acc: 0.8887\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2552 - acc: 0.8954 - val_loss: 0.2792 - val_acc: 0.8786\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2601 - acc: 0.8903 - val_loss: 0.2598 - val_acc: 0.8899\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2543 - acc: 0.8954 - val_loss: 0.2502 - val_acc: 0.8949\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2479 - acc: 0.8977 - val_loss: 0.2491 - val_acc: 0.8969\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2512 - acc: 0.8957 - val_loss: 0.2541 - val_acc: 0.8944\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2564 - acc: 0.8945 - val_loss: 0.2568 - val_acc: 0.8934\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2508 - acc: 0.8963 - val_loss: 0.2547 - val_acc: 0.8907\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2520 - acc: 0.8954 - val_loss: 0.2629 - val_acc: 0.8933\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2496 - acc: 0.8956 - val_loss: 0.2490 - val_acc: 0.8952\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2559 - acc: 0.8934 - val_loss: 0.2579 - val_acc: 0.8926\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2518 - acc: 0.8945 - val_loss: 0.2533 - val_acc: 0.8890\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2498 - acc: 0.8960 - val_loss: 0.2567 - val_acc: 0.8895\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2521 - acc: 0.8945 - val_loss: 0.2544 - val_acc: 0.8943\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2540 - acc: 0.8941 - val_loss: 0.2688 - val_acc: 0.8932\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2535 - acc: 0.8971 - val_loss: 0.2518 - val_acc: 0.8944\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2514 - acc: 0.8954 - val_loss: 0.2550 - val_acc: 0.8926\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2540 - acc: 0.8960 - val_loss: 0.2568 - val_acc: 0.8918\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2544 - acc: 0.8933 - val_loss: 0.2535 - val_acc: 0.8930\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2613 - acc: 0.8900 - val_loss: 0.2657 - val_acc: 0.8950\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2718 - acc: 0.8848 - val_loss: 0.2671 - val_acc: 0.8933\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2738 - acc: 0.8795 - val_loss: 0.2651 - val_acc: 0.8882\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2556 - acc: 0.8963 - val_loss: 0.2620 - val_acc: 0.8831\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2545 - acc: 0.8961 - val_loss: 0.2599 - val_acc: 0.8930\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2674 - acc: 0.8894 - val_loss: 0.2802 - val_acc: 0.8806\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2608 - acc: 0.8902 - val_loss: 0.2526 - val_acc: 0.8918\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2567 - acc: 0.8929 - val_loss: 0.2775 - val_acc: 0.8787\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2656 - acc: 0.8903 - val_loss: 0.2608 - val_acc: 0.8909\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2707 - acc: 0.8863 - val_loss: 0.2597 - val_acc: 0.8890\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2574 - acc: 0.8924 - val_loss: 0.2563 - val_acc: 0.8900\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2603 - acc: 0.8902 - val_loss: 0.2658 - val_acc: 0.8932\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2560 - acc: 0.8951 - val_loss: 0.2653 - val_acc: 0.8813\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2606 - acc: 0.8907 - val_loss: 0.2709 - val_acc: 0.8920\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2596 - acc: 0.8931 - val_loss: 0.2726 - val_acc: 0.8853\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2654 - acc: 0.8899 - val_loss: 0.2609 - val_acc: 0.8862\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2556 - acc: 0.8954 - val_loss: 0.2701 - val_acc: 0.8871\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2646 - acc: 0.8900 - val_loss: 0.2671 - val_acc: 0.8887\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2592 - acc: 0.8889 - val_loss: 0.2569 - val_acc: 0.8930\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2512 - acc: 0.8963 - val_loss: 0.2521 - val_acc: 0.8932\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2580 - acc: 0.8955 - val_loss: 0.2636 - val_acc: 0.8877\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2592 - acc: 0.8927 - val_loss: 0.2600 - val_acc: 0.8932\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2635 - acc: 0.8888 - val_loss: 0.2614 - val_acc: 0.8876\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2588 - acc: 0.8938 - val_loss: 0.2771 - val_acc: 0.8853\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2710 - acc: 0.8895 - val_loss: 0.3047 - val_acc: 0.8822\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2643 - acc: 0.8932 - val_loss: 0.2669 - val_acc: 0.8833\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2552 - acc: 0.8897 - val_loss: 0.2535 - val_acc: 0.8855\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2581 - acc: 0.8936 - val_loss: 0.2642 - val_acc: 0.8901\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2625 - acc: 0.8919 - val_loss: 0.2815 - val_acc: 0.8871\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2630 - acc: 0.8927 - val_loss: 0.2799 - val_acc: 0.8755\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2578 - acc: 0.8974 - val_loss: 0.2531 - val_acc: 0.8930\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2602 - acc: 0.8899 - val_loss: 0.2712 - val_acc: 0.8737\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2593 - acc: 0.8956 - val_loss: 0.2640 - val_acc: 0.8878\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2551 - acc: 0.8957 - val_loss: 0.2651 - val_acc: 0.8882\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.2724 - acc: 0.8841 - val_loss: 0.2775 - val_acc: 0.8937\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2708 - acc: 0.8946 - val_loss: 0.2656 - val_acc: 0.8945\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2605 - acc: 0.8926 - val_loss: 0.2587 - val_acc: 0.8925\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2552 - acc: 0.8965 - val_loss: 0.2559 - val_acc: 0.8920\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2590 - acc: 0.8931 - val_loss: 0.2596 - val_acc: 0.8809\n",
      "Confusion matrices:\n",
      "---------------------\n",
      "[[30616  2436]\n",
      " [ 2260  5418]]\n",
      "[[13079  1086]\n",
      " [ 1023  2268]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.926297954738\n",
      "Train Recall:  0.705652513675\n",
      "Train Precision:  0.689839572193\n",
      "Train Accuracy:  0.884704149276\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.923332156724\n",
      "Test Recall:  0.689152233364\n",
      "Test Precision:  0.676207513417\n",
      "Test Accuracy:  0.879181943171\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  0.1\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 0.3234 - acc: 0.8452 - val_loss: 0.2942 - val_acc: 0.8731\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2998 - acc: 0.8604 - val_loss: 0.3407 - val_acc: 0.8086\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2977 - acc: 0.8652 - val_loss: 0.3776 - val_acc: 0.7981\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3004 - acc: 0.8638 - val_loss: 0.2807 - val_acc: 0.8788\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2911 - acc: 0.8697 - val_loss: 0.3170 - val_acc: 0.8689\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3018 - acc: 0.8604 - val_loss: 0.2844 - val_acc: 0.8839\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2855 - acc: 0.8708 - val_loss: 0.2893 - val_acc: 0.8601\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2905 - acc: 0.8713 - val_loss: 0.3186 - val_acc: 0.8069\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3312 - acc: 0.8373 - val_loss: 0.3220 - val_acc: 0.8431\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3261 - acc: 0.8425 - val_loss: 0.3061 - val_acc: 0.8470\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3050 - acc: 0.8574 - val_loss: 0.2976 - val_acc: 0.8770\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3081 - acc: 0.8542 - val_loss: 0.2827 - val_acc: 0.8793\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2844 - acc: 0.8792 - val_loss: 0.3357 - val_acc: 0.8073\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3002 - acc: 0.8595 - val_loss: 0.3122 - val_acc: 0.8082\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3206 - acc: 0.8462 - val_loss: 0.3064 - val_acc: 0.8792\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3051 - acc: 0.8558 - val_loss: 0.3418 - val_acc: 0.8082\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2969 - acc: 0.8639 - val_loss: 0.3106 - val_acc: 0.8520\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3031 - acc: 0.8599 - val_loss: 0.3061 - val_acc: 0.8607\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3088 - acc: 0.8554 - val_loss: 0.3014 - val_acc: 0.8871\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3084 - acc: 0.8656 - val_loss: 0.2989 - val_acc: 0.8895\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3329 - acc: 0.8404 - val_loss: 0.3821 - val_acc: 0.8168\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3274 - acc: 0.8508 - val_loss: 0.3186 - val_acc: 0.8744\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3076 - acc: 0.8658 - val_loss: 0.3729 - val_acc: 0.8048\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3248 - acc: 0.8437 - val_loss: 0.3018 - val_acc: 0.8761\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3226 - acc: 0.8396 - val_loss: 0.3260 - val_acc: 0.8575\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3003 - acc: 0.8656 - val_loss: 0.3067 - val_acc: 0.8597\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3110 - acc: 0.8661 - val_loss: 0.2986 - val_acc: 0.8817\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3316 - acc: 0.8458 - val_loss: 0.3058 - val_acc: 0.8330\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2905 - acc: 0.8713 - val_loss: 0.3051 - val_acc: 0.8674\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3066 - acc: 0.8624 - val_loss: 0.3752 - val_acc: 0.7336\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3132 - acc: 0.8531 - val_loss: 0.2993 - val_acc: 0.8653\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2984 - acc: 0.8652 - val_loss: 0.2960 - val_acc: 0.8753\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3166 - acc: 0.8448 - val_loss: 0.3143 - val_acc: 0.8831\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2909 - acc: 0.8720 - val_loss: 0.3081 - val_acc: 0.8387\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3399 - acc: 0.8325 - val_loss: 0.3645 - val_acc: 0.8063\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3478 - acc: 0.8167 - val_loss: 0.3321 - val_acc: 0.8181\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3045 - acc: 0.8512 - val_loss: 0.2898 - val_acc: 0.8747\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3106 - acc: 0.8623 - val_loss: 0.3811 - val_acc: 0.8240\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3000 - acc: 0.8656 - val_loss: 0.3349 - val_acc: 0.8699\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2903 - acc: 0.8803 - val_loss: 0.3010 - val_acc: 0.8629\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3039 - acc: 0.8534 - val_loss: 0.2967 - val_acc: 0.8733\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2934 - acc: 0.8743 - val_loss: 0.2859 - val_acc: 0.8806\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2904 - acc: 0.8747 - val_loss: 0.3825 - val_acc: 0.8054\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3599 - acc: 0.8158 - val_loss: 0.3521 - val_acc: 0.8070\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3375 - acc: 0.8272 - val_loss: 0.3417 - val_acc: 0.8202\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3187 - acc: 0.8508 - val_loss: 0.3284 - val_acc: 0.7970\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2965 - acc: 0.8670 - val_loss: 0.2780 - val_acc: 0.8846\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.2940 - acc: 0.8611 - val_loss: 0.2962 - val_acc: 0.8689\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3253 - acc: 0.8348 - val_loss: 0.3217 - val_acc: 0.8301\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3101 - acc: 0.8534 - val_loss: 0.2838 - val_acc: 0.8857\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3167 - acc: 0.8573 - val_loss: 0.3610 - val_acc: 0.8203\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3565 - acc: 0.8180 - val_loss: 0.3588 - val_acc: 0.8140\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3366 - acc: 0.8339 - val_loss: 0.3328 - val_acc: 0.8318\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3190 - acc: 0.8441 - val_loss: 0.3591 - val_acc: 0.8049\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3388 - acc: 0.8249 - val_loss: 0.3267 - val_acc: 0.8416\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3020 - acc: 0.8605 - val_loss: 0.3422 - val_acc: 0.8399\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3018 - acc: 0.8644 - val_loss: 0.3223 - val_acc: 0.8738\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3053 - acc: 0.8748 - val_loss: 0.2803 - val_acc: 0.8842\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2882 - acc: 0.8750 - val_loss: 0.2877 - val_acc: 0.8822\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2859 - acc: 0.8758 - val_loss: 0.3588 - val_acc: 0.8181\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2907 - acc: 0.8719 - val_loss: 0.2778 - val_acc: 0.8912\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2949 - acc: 0.8605 - val_loss: 0.3471 - val_acc: 0.8051\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2984 - acc: 0.8582 - val_loss: 0.3001 - val_acc: 0.8747\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2817 - acc: 0.8827 - val_loss: 0.2834 - val_acc: 0.8776\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2882 - acc: 0.8784 - val_loss: 0.3812 - val_acc: 0.8097\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3144 - acc: 0.8555 - val_loss: 0.2763 - val_acc: 0.8890\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2785 - acc: 0.8854 - val_loss: 0.2784 - val_acc: 0.8806\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2822 - acc: 0.8811 - val_loss: 0.2900 - val_acc: 0.8782\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3247 - acc: 0.8499 - val_loss: 0.2914 - val_acc: 0.8764\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3362 - acc: 0.8394 - val_loss: 0.3913 - val_acc: 0.8121\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3372 - acc: 0.8418 - val_loss: 0.3115 - val_acc: 0.8051\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3257 - acc: 0.8510 - val_loss: 0.3024 - val_acc: 0.8613\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3473 - acc: 0.8288 - val_loss: 0.3926 - val_acc: 0.8047\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3759 - acc: 0.8140 - val_loss: 0.3804 - val_acc: 0.8041\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3733 - acc: 0.8141 - val_loss: 0.3820 - val_acc: 0.8033\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3727 - acc: 0.8144 - val_loss: 0.3795 - val_acc: 0.8049\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3723 - acc: 0.8144 - val_loss: 0.3772 - val_acc: 0.8048\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3714 - acc: 0.8147 - val_loss: 0.3764 - val_acc: 0.8067\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3696 - acc: 0.8162 - val_loss: 0.3741 - val_acc: 0.8097\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3581 - acc: 0.8281 - val_loss: 0.3024 - val_acc: 0.8808\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.2819 - val_acc: 0.8847\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2830 - acc: 0.8801 - val_loss: 0.2790 - val_acc: 0.8869\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2812 - acc: 0.8846 - val_loss: 0.2761 - val_acc: 0.8887\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2770 - acc: 0.8891 - val_loss: 0.2825 - val_acc: 0.8793\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.2793 - acc: 0.8805 - val_loss: 0.2772 - val_acc: 0.8903\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3277 - acc: 0.8456 - val_loss: 0.3512 - val_acc: 0.8098\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3307 - acc: 0.8234 - val_loss: 0.2852 - val_acc: 0.8796\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.2840 - acc: 0.8814 - val_loss: 0.2745 - val_acc: 0.8918\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3380 - acc: 0.8311 - val_loss: 0.3044 - val_acc: 0.8648\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3045 - acc: 0.8636 - val_loss: 0.2929 - val_acc: 0.8850\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2960 - acc: 0.8637 - val_loss: 0.2942 - val_acc: 0.8637\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3136 - acc: 0.8511 - val_loss: 0.3525 - val_acc: 0.8054\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3151 - acc: 0.8391 - val_loss: 0.2879 - val_acc: 0.8785\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3025 - acc: 0.8694 - val_loss: 0.2875 - val_acc: 0.8705\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.2932 - acc: 0.8738 - val_loss: 0.3882 - val_acc: 0.8048\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3751 - acc: 0.8139 - val_loss: 0.3816 - val_acc: 0.8044\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3726 - acc: 0.8140 - val_loss: 0.3792 - val_acc: 0.8046\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3696 - acc: 0.8140 - val_loss: 0.3752 - val_acc: 0.8054\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3644 - acc: 0.8179 - val_loss: 0.3692 - val_acc: 0.8129\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3611 - acc: 0.8204 - val_loss: 0.3734 - val_acc: 0.8137\n",
      "Confusion matrices:\n",
      "---------------------\n",
      "[[32744   308]\n",
      " [ 7037   641]]\n",
      "[[14009   156]\n",
      " [ 3027   264]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.990681350599\n",
      "Train Recall:  0.0834852826257\n",
      "Train Precision:  0.675447839831\n",
      "Train Accuracy:  0.819666093788\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.98898693964\n",
      "Test Recall:  0.0802187784868\n",
      "Test Precision:  0.628571428571\n",
      "Test Accuracy:  0.817655820348\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  1\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 0.3441 - acc: 0.8195 - val_loss: 0.4215 - val_acc: 0.8048\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3553 - acc: 0.8120 - val_loss: 0.3262 - val_acc: 0.8396\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3443 - acc: 0.8230 - val_loss: 0.6958 - val_acc: 0.8052\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3426 - acc: 0.8180 - val_loss: 0.5848 - val_acc: 0.6670\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3652 - acc: 0.8133 - val_loss: 0.3709 - val_acc: 0.8049\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3532 - acc: 0.8164 - val_loss: 0.4439 - val_acc: 0.8055\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3382 - acc: 0.8235 - val_loss: 0.3749 - val_acc: 0.7960\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3372 - acc: 0.8308 - val_loss: 0.5362 - val_acc: 0.8052\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3674 - acc: 0.8099 - val_loss: 0.7332 - val_acc: 0.8048\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3461 - acc: 0.8330 - val_loss: 0.8379 - val_acc: 0.6045\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3508 - acc: 0.8340 - val_loss: 0.3601 - val_acc: 0.8057\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3832 - acc: 0.8122 - val_loss: 0.4620 - val_acc: 0.8051\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3550 - acc: 0.8366 - val_loss: 0.3186 - val_acc: 0.8442\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3874 - acc: 0.8058 - val_loss: 0.8898 - val_acc: 0.6478\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3909 - acc: 0.8016 - val_loss: 0.3142 - val_acc: 0.8467\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.4203 - acc: 0.7862 - val_loss: 0.3670 - val_acc: 0.8085\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3730 - acc: 0.8143 - val_loss: 0.4868 - val_acc: 0.7601\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3445 - acc: 0.8264 - val_loss: 0.2854 - val_acc: 0.8648\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3858 - acc: 0.8077 - val_loss: 0.3526 - val_acc: 0.7949\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3775 - acc: 0.8080 - val_loss: 0.4465 - val_acc: 0.8055\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 0.3464 - acc: 0.8326 - val_loss: 0.3611 - val_acc: 0.8481\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3351 - acc: 0.8328 - val_loss: 0.3426 - val_acc: 0.8696\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3104 - acc: 0.8548 - val_loss: 0.3190 - val_acc: 0.8330\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3068 - acc: 0.8557 - val_loss: 0.3236 - val_acc: 0.8132\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 0.3088 - acc: 0.8568 - val_loss: 0.3276 - val_acc: 0.8585\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3175 - acc: 0.8548 - val_loss: 0.6380 - val_acc: 0.8048\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3620 - acc: 0.8110 - val_loss: 0.4023 - val_acc: 0.8156\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 0.3444 - acc: 0.8194 - val_loss: 0.3296 - val_acc: 0.8330\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.3541 - acc: 0.8139 - val_loss: 0.4723 - val_acc: 0.8063\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3333 - acc: 0.8394 - val_loss: 0.7382 - val_acc: 0.6992\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3379 - acc: 0.8255 - val_loss: 0.4471 - val_acc: 0.8055\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3534 - acc: 0.8225 - val_loss: 0.4384 - val_acc: 0.8057\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3452 - acc: 0.8235 - val_loss: 0.3253 - val_acc: 0.8315\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3470 - acc: 0.8330 - val_loss: 0.5590 - val_acc: 0.6943\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3891 - acc: 0.7931 - val_loss: 0.8453 - val_acc: 0.6760\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3720 - acc: 0.8044 - val_loss: 1.2017 - val_acc: 0.5956\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3616 - acc: 0.8034 - val_loss: 0.4752 - val_acc: 0.8058\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3374 - acc: 0.8250 - val_loss: 0.3496 - val_acc: 0.8086\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3186 - acc: 0.8454 - val_loss: 0.3637 - val_acc: 0.8068\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3177 - acc: 0.8449 - val_loss: 0.3295 - val_acc: 0.8116\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3204 - acc: 0.8486 - val_loss: 0.6608 - val_acc: 0.6403\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3271 - acc: 0.8315 - val_loss: 0.4815 - val_acc: 0.7222\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3576 - acc: 0.8088 - val_loss: 0.6861 - val_acc: 0.6975\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 1s 20us/step - loss: 0.3706 - acc: 0.7928 - val_loss: 0.3612 - val_acc: 0.7458\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3560 - acc: 0.8099 - val_loss: 0.4235 - val_acc: 0.7258\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 1s 20us/step - loss: 0.3717 - acc: 0.7931 - val_loss: 0.3864 - val_acc: 0.7393\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3576 - acc: 0.8024 - val_loss: 0.3029 - val_acc: 0.8713\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.3105 - acc: 0.8546 - val_loss: 0.3065 - val_acc: 0.8781\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3349 - acc: 0.8384 - val_loss: 0.3763 - val_acc: 0.8117\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3884 - acc: 0.7912 - val_loss: 0.4182 - val_acc: 0.8051\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3845 - acc: 0.7900 - val_loss: 0.3585 - val_acc: 0.8113\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.3750 - acc: 0.7982 - val_loss: 0.3917 - val_acc: 0.8081\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.3233 - acc: 0.8408 - val_loss: 0.3136 - val_acc: 0.8531\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3297 - acc: 0.8336 - val_loss: 0.3204 - val_acc: 0.8571\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3085 - acc: 0.8475 - val_loss: 0.2894 - val_acc: 0.8737\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.3032 - acc: 0.8620 - val_loss: 0.5607 - val_acc: 0.8046\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3299 - acc: 0.8423 - val_loss: 0.5026 - val_acc: 0.8122\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3104 - acc: 0.8583 - val_loss: 0.3125 - val_acc: 0.8388\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3277 - acc: 0.8431 - val_loss: 0.3051 - val_acc: 0.8489\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3047 - acc: 0.8608 - val_loss: 0.5629 - val_acc: 0.6854\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 1s 20us/step - loss: 0.3327 - acc: 0.8288 - val_loss: 0.3317 - val_acc: 0.7981\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3165 - acc: 0.8405 - val_loss: 0.3115 - val_acc: 0.8365\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3323 - acc: 0.8345 - val_loss: 0.5937 - val_acc: 0.8515\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3552 - acc: 0.8096 - val_loss: 0.4731 - val_acc: 0.7999\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 1s 20us/step - loss: 0.3873 - acc: 0.8129 - val_loss: 0.4444 - val_acc: 0.8094\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3799 - acc: 0.8129 - val_loss: 0.4397 - val_acc: 0.8103\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3706 - acc: 0.8264 - val_loss: 0.6229 - val_acc: 0.8085\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3445 - acc: 0.8331 - val_loss: 1.1007 - val_acc: 0.6166\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3603 - acc: 0.8200 - val_loss: 0.3607 - val_acc: 0.8157\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 1s 15us/step - loss: 0.3450 - acc: 0.8314 - val_loss: 0.5104 - val_acc: 0.6693\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3235 - acc: 0.8454 - val_loss: 0.4110 - val_acc: 0.8054\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3054 - acc: 0.8622 - val_loss: 0.3325 - val_acc: 0.8497\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.2977 - acc: 0.8694 - val_loss: 0.3227 - val_acc: 0.8055\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3086 - acc: 0.8516 - val_loss: 0.3222 - val_acc: 0.8542\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3443 - acc: 0.8261 - val_loss: 0.5101 - val_acc: 0.6930\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3276 - acc: 0.8401 - val_loss: 0.3620 - val_acc: 0.8068\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3632 - acc: 0.8041 - val_loss: 0.4644 - val_acc: 0.8053\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3552 - acc: 0.8099 - val_loss: 0.3266 - val_acc: 0.8285\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3264 - acc: 0.8462 - val_loss: 0.5002 - val_acc: 0.8051\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 0.3521 - acc: 0.8188 - val_loss: 0.3581 - val_acc: 0.8062\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3410 - acc: 0.8281 - val_loss: 0.4086 - val_acc: 0.8059\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3738 - acc: 0.8048 - val_loss: 0.3895 - val_acc: 0.8705\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3715 - acc: 0.8096 - val_loss: 0.3729 - val_acc: 0.7510\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 0.3766 - acc: 0.8035 - val_loss: 0.5813 - val_acc: 0.6937\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3816 - acc: 0.8003 - val_loss: 0.3867 - val_acc: 0.7064\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 0.3985 - acc: 0.7861 - val_loss: 0.3982 - val_acc: 0.8051\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3993 - acc: 0.7903 - val_loss: 0.4706 - val_acc: 0.7010\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 0.3958 - acc: 0.7902 - val_loss: 0.4091 - val_acc: 0.8049\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3860 - acc: 0.8016 - val_loss: 0.7153 - val_acc: 0.7132\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3767 - acc: 0.8103 - val_loss: 0.7473 - val_acc: 0.7213\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3186 - acc: 0.8600 - val_loss: 0.3657 - val_acc: 0.8055\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3221 - acc: 0.8590 - val_loss: 0.3126 - val_acc: 0.8555\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.3154 - acc: 0.8501 - val_loss: 0.4860 - val_acc: 0.7399\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3075 - acc: 0.8592 - val_loss: 0.4946 - val_acc: 0.8494\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.3101 - acc: 0.8635 - val_loss: 0.3171 - val_acc: 0.8354\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3593 - acc: 0.8103 - val_loss: 0.6551 - val_acc: 0.7443\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 0.3535 - acc: 0.8249 - val_loss: 0.3744 - val_acc: 0.7765\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3208 - acc: 0.8435 - val_loss: 0.9292 - val_acc: 0.8049\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3419 - acc: 0.8434 - val_loss: 0.3629 - val_acc: 0.8060\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 0.3907 - acc: 0.7982 - val_loss: 0.4289 - val_acc: 0.8063\n",
      "Confusion matrices:\n",
      "---------------------\n",
      "[[33012    40]\n",
      " [ 7574   104]]\n",
      "[[14147    18]\n",
      " [ 3247    44]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.998789785792\n",
      "Train Recall:  0.013545194061\n",
      "Train Precision:  0.722222222222\n",
      "Train Accuracy:  0.813061625338\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.998729262266\n",
      "Test Recall:  0.0133697964145\n",
      "Test Precision:  0.709677419355\n",
      "Test Accuracy:  0.812958295142\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  10\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 1s 29us/step - loss: 3.0075 - acc: 0.8119 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 1s 21us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 1s 20us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 1s 18us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 1s 21us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 1s 17us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 1s 16us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 1s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 1s 19us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s 14us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s 15us/step - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Confusion matrices:\n",
      "---------------------\n",
      "[[33052     0]\n",
      " [ 7678     0]]\n",
      "[[14165     0]\n",
      " [ 3291     0]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  1.0\n",
      "Train Recall:  0.0\n",
      "Train Precision:  nan\n",
      "Train Accuracy:  0.811490301989\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  1.0\n",
      "Test Recall:  0.0\n",
      "Test Precision:  nan\n",
      "Test Accuracy:  0.81146883593\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialiations for plotting later on\n",
    "train_recall = []\n",
    "test_recall = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "# Model for all learning rates\n",
    "for i in range(-5,2,1):\n",
    "    eta = 10**i\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Modelling for learning rate - \", eta)\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    \n",
    "    # Custom optimizer based on SGD\n",
    "    SGD = optimizers.SGD(lr = eta)\n",
    "    \n",
    "    # Modelling\n",
    "    ann_model_lrExp = Sequential()\n",
    "\n",
    "    ann_model_lrExp.add(Dense(29, input_dim=21, activation='sigmoid', kernel_initializer='normal'))\n",
    "    ann_model_lrExp.add(Dense(1, activation='sigmoid', kernel_initializer='normal'))\n",
    "\n",
    "    ann_model_lrExp.compile(loss='binary_crossentropy', optimizer=SGD, metrics=['accuracy'])\n",
    "\n",
    "    ann_model_lrExp.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2)\n",
    "\n",
    "    # Predictions\n",
    "    train_pred = ann_model_lrExp.predict_classes(X_train)\n",
    "    test_pred = ann_model_lrExp.predict_classes(X_test)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    confusion_matrix_train = confusion_matrix(y_train, train_pred)\n",
    "    confusion_matrix_test = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "    print(\"Confusion matrices:\")\n",
    "    print(\"---------------------\")\n",
    "    print(confusion_matrix_train)\n",
    "    print(confusion_matrix_test)\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    # Metrics on train data\n",
    "    # Accuracy\n",
    "    accuracy_Train_lrExp = (confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "    # Specificity or true negative rate (TNR)\n",
    "    specificity_Train_lrExp = confusion_matrix_train[0,0]/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\n",
    "    # Sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "    recall_Train_lrExp = confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "    # Precision\n",
    "    precision_Train_lrExp = confusion_matrix_train[1,1]/(confusion_matrix_train[0,1]+confusion_matrix_train[1,1])\n",
    "\n",
    "    print(\"Evaluation metrics on train data for ann_lrExp model:\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Train Specificity: \",specificity_Train_lrExp)\n",
    "    print(\"Train Recall: \",recall_Train_lrExp)\n",
    "    print(\"Train Precision: \",precision_Train_lrExp)\n",
    "    print(\"Train Accuracy: \",accuracy_Train_lrExp)\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "    # Metrics on test data\n",
    "    #Accuracy\n",
    "    accuracy_Test_lrExp = (confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "    # Specificity or true negative rate (TNR)\n",
    "    specificity_Test_lrExp = confusion_matrix_test[0,0]/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1])\n",
    "    # Sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "    recall_Test_lrExp = confusion_matrix_test[1,1]/(confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "    # Precision\n",
    "    precision_Test_lrExp = confusion_matrix_test[1,1]/(confusion_matrix_test[0,1]+confusion_matrix_test[1,1])\n",
    "\n",
    "    print(\"Evaluation metrics on test data for ann_lrExp model:\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Test Specificity: \",specificity_Test_lrExp)\n",
    "    print(\"Test Recall: \",recall_Test_lrExp)\n",
    "    print(\"Test Precision: \",precision_Test_lrExp)\n",
    "    print(\"Test Accuracy: \",accuracy_Test_lrExp)\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "    # Plotting realted variables\n",
    "    train_recall.append(recall_Train_lrExp)\n",
    "    test_recall.append(recall_Test_lrExp)\n",
    "    train_accuracy.append(accuracy_Train_lrExp)\n",
    "    test_accuracy.append(accuracy_Test_lrExp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEaCAYAAADg2nttAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMSaQFQtjjDGB/gu13ws2yDa2+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c2447a710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting accuracy for various learning rates for 100 epochs\n",
    "plt.plot([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], train_accuracy)\n",
    "plt.plot([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], test_accuracy)\n",
    "plt.title('Accuracy for various learning rates')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Learning Rates')\n",
    "plt.xscale('log')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBto2egAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8be7fa15f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting recalls for various learning rates for 100 epochs\n",
    "plt.plot([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], train_recall)\n",
    "plt.plot([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], test_recall)\n",
    "plt.title('Recall for various learning rates')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Learning Rates')\n",
    "plt.xscale('log')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
