{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Art of Training Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where Neural Networks and Deep Learning Fit\n",
    "\n",
    "* Deep Learning and neural networks are a subfield of Machine Learning and Artificial Intelligence in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/deep_learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Machine Learning, we see that rules are learned from data, and in deep learning the same holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/classical_vs_ml.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning representations from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In traditional machine learning we see that changing the representation of the data helps ease the process of learning from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/learning_representations.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will walk through the fundamentals of both neural networks and keras together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers as a fundamental building block of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Layers act as data filters or distillers, distilling and generating important information from the input to get to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/learning_representations_mlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you guys remember the MNIST dataset, the four layers above have \"learnt\" the following ways to represent the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/learning_representations_mlp_02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So basically, each layer takes input as data and spits out data as output, simple as that. Now, let's dive into the details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of training neural networks is to find these perfect representation of data, which we get by \"learning\" the right weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/learning_weights.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before we go ahead and understand how to learn the perfect weights, we have to understand what a layer does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are different categories of layers in Keras, the most commonly used ones are \"Core Layers\", \"Convolution Layers\", \"Recurrent Layers\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1 = Dense(units = 32, activation = 'sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's break down the above layer that we just built using the Dense class from keras' core layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The OUTPUT from the above layer can be given by sigmoid ( dot ( W, INPUT ) + b ), in the first round of training the W (weights) are randomly \"initialized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every input to the Dense layer (also known as fully connected) is connected to every unit in the hidden layer, as shown in the figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/fc_dense_layers_keras.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will see many more categories and types of layer objects in keras, the \"Dense\" core layer is just one such class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the layers together : The Keras Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The keras sequential api enables us to build common yet complex neural network architectures flexibly\n",
    "\n",
    "* Objects of the Keras sequential class, can have multiple neural network layers stacked on top of one another\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/keras_sequential_api.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can create a Keras sequential model by passing in a list of layers to the Sequential object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,), activation = 'sigmoid'),\n",
    "    Dense(10, activation = 'sigmoid'),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But soon this becomes a problem to add more layers, so the \"add\" method on the sequential class object can add layers sequentially to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_network = Sequential()\n",
    "\n",
    "neural_network.add(Dense(32, input_dim=784, activation = 'sigmoid'))\n",
    "\n",
    "neural_network.add(Dense(10, activation = 'sigmoid'))\n",
    "\n",
    "neural_network.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The summary method on the neural network object gives us basic information about the structure of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 25,461\n",
      "Trainable params: 25,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neural_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all! Keras is that simple, initialize an object of the Sequential class, use the add method on that object to add layers to the network sequentially, but wait what about the loss function? what about the optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining, Compiling and Running a Neural Network in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The process of learning in a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The loss score is feedback signal that says how far is the output of your network compared to the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/loss_function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two such loss scores that we use quite frequently are :\n",
    "    \n",
    "    1) Binary Cross-Entropy: For Two-Class Classification problems\n",
    "    \n",
    "    2) Mean Squared Error: For Regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have already come across mean squared error before, so let's dive deeper into Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray} \n",
    "  C = -\\frac{1}{n} \\sum_x \\left[y \\ln p + (1-y ) \\ln (1-p) \\right]\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the above equation, p is the output of the network, n is the total number of items of training data, the sum is over all training inputs, x, and y is the corresponding desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below, we see the value of the cross entropy (sometimes referred to as the log loss) changing with the predicted probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/binary_cross_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An optimizer is an algorithm that uses this feedback signal, to actually update the weights so that the output from the network gets closer to the ground truth. The first optimizer that we use is Stochastic Gradient Descent (SGD), we will slowly come across many more optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/optimizer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can import Classes from the optimizers module of keras and customize the specific optimizer to our liking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "customized_optimizer = SGD(lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We already know how learning rate can effect convergence, the graph below provides a decent intuition, hence haing the flexibility to change the learning rate is very important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/learning_rate.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the neural network ( loss function + optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_network.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_network.compile(loss = 'binary_crossentropy', optimizer = customized_optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see from the compile step above we need to specify the loss function, optimization algorithm and we can also mention the metrics that we want to monitor while training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Also, please __note__ that the optimizer argument can either be a string or a customizable object from the optimizers module in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So are we done? what about learning the weights? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training the network in Keras is also very simple, we call the \"fit\" method and pass in the arguments\n",
    "\n",
    "* Some important terms for training neural networks are epochs, batch_size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NOTE : Don't run the following line of code as we do not yet have X_train and y_train\n",
    "\n",
    "neural_network.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An __Epoch__ is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n",
    "\n",
    "* Since most of the times an epoch is too large to fit in memory, we divide the data into batches and compute the gradient on batches for each forward and backward pass\n",
    "\n",
    "* __Batch size__ is the number of samples that are going to be propagated through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/learning_rate_epochs_rel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tensors, the oil for the deep learning engine"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> import numpy as np\n",
    ">>> x = np.array(12)\n",
    ">>> x\n",
    "array(12)\n",
    ">>> x.ndim\n",
    "0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> x1d = np.array([0, 1, 2, 3])\n",
    "\n",
    ">>> x1d.ndim\n",
    "1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Please note the zeroes before the integers would raise an error in python, they are here only to make indexing clear\n",
    "\n",
    ">>> x2d = np.array([[00, 01, 02, 03],\n",
    "                  [10, 11, 12, 13],\n",
    "                  [20, 21, 22, 23]])\n",
    "                  \n",
    ">>> x2d.ndim\n",
    "2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Please note the zeroes before the integers would raise an error in python, they are here only to make indexing clear\n",
    "\n",
    ">>> x3d = np.array([[[000, 001, 002, 003, 004],\n",
    "                   [010, 011, 012, 013, 014],\n",
    "                   [020, 021, 022, 023, 024]],\n",
    "                  [[100, 101, 102, 103, 104],\n",
    "                   [110, 111, 112, 113, 114],\n",
    "                   [120, 121, 122, 123, 124]],\n",
    "                  [[200, 201, 202, 203, 204],\n",
    "                   [210, 211, 212, 213, 214],\n",
    "                   [220, 221, 222, 223, 224]]])\n",
    "                   \n",
    ">>> x3d.ndim\n",
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](img/4d_array.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
