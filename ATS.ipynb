{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resume Phrase Matcher code\n",
    "\n",
    "\n",
    "#importing all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import PyPDF2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')  \n",
    "\n",
    "#spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read resumes from the folder one by one\n",
    "mypath='/Users/babysupriyavalikala/Documents/CandidateResume/' #enter your path here where you saved the resumes\n",
    "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "\n",
    "\n",
    "#For mypath, the documents were in pdf format.\n",
    "\n",
    "#The onlyfiles line checks whether the address of the folder is valid \n",
    "#and does contain the documents and if yes, then it iterated over the pdf \n",
    "#documents one by one.\n",
    "#every f is a resume(file) here \n",
    "#braces indicarte list here --and condition to store in list is given \n",
    "#if File is in path and file is a resume(pdf)--it gets stored in the list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfextract(file):\n",
    "    fileReader = PyPDF2.PdfFileReader(open(file,'rb'))  #it reads three files one by one\n",
    "    countpage = fileReader.getNumPages()                #number of pages in a file\n",
    "    count = 0\n",
    "    text = []\n",
    "    while count < countpage:    \n",
    "        pageObj = fileReader.getPage(count)          #iterating every page content here   \n",
    "        count +=1                                  \n",
    "        t = pageObj.extractText()                   #extracting text from each page--and adding to text list \n",
    "        \n",
    "        print (t)\n",
    "        text.append(t)\n",
    "    return text                 #text is a LIST - it contains the text of first resume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that does phrase matching and builds a candidate profile\n",
    "\n",
    "#it does segregation\n",
    "def create_profile(file):\n",
    "    text = pdfextract(file) \n",
    "    text = str(text)              #converting the string to text\n",
    "    text = text.replace(\"\\\\n\", \"\")#replace \\\\n with NULL\n",
    "     \n",
    "    text = text.lower()           #text to lower case\n",
    "    #below is the csv where we have all the keywords, you can customize your own\n",
    "    a=csv.reader('template.csv',delimiter=',')\n",
    "    keyword_dict =a\n",
    "    stats_words = [nlp(text) for text in keyword_dict['Statistics'].dropna(axis = 0)]\n",
    "    #list   dropna--drop column if the value is NA(not arithmetic)\n",
    "    NLP_words = [nlp(text) for text in keyword_dict['NLP'].dropna(axis = 0)]\n",
    "    ML_words = [nlp(text) for text in keyword_dict['Machine Learning'].dropna(axis = 0)]\n",
    "    DL_words = [nlp(text) for text in keyword_dict['Deep Learning'].dropna(axis = 0)]\n",
    "    R_words = [nlp(text) for text in keyword_dict['R Language'].dropna(axis = 0)]\n",
    "    python_words = [nlp(text) for text in keyword_dict['Python Language'].dropna(axis = 0)]\n",
    "    Data_Engineering_words = [nlp(text) for text in keyword_dict['Data Engineering'].dropna(axis = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffStatistics', 'Machine Learning', 'Deep Learning', 'R Language', 'Python Language', 'NLP', 'Data Engineering']\n",
      "['statistical models', 'linear regression', 'neural network', 'r', 'python', 'nlp', 'aws']\n",
      "['statistical modeling', 'logistic regression', 'keras', 'ggplot', 'flask', 'natural language processing', 'ec2']\n",
      "['probability', 'K means', 'theano', 'shiny', 'django', 'topic modeling', 'amazon redshift']\n",
      "['normal distribution', 'random forest ', 'face detection ', 'cran', 'pandas', 'lda', 's3']\n",
      "['poisson distribution', 'xgboost', 'neural networks', 'dplyr', 'numpy', 'named entity recognition', 'docker']\n",
      "['survival models', 'svm', 'convolutional neural network (cnn)', 'tidyr', 'scikitlearn', 'pos tagging', 'kubernetes']\n",
      "['hypothesis testing', 'naïve bayes', 'recurrent neural network(RNN)', 'lubridate', 'sklearn ', 'word2vec', 'scala']\n",
      "['bayesian inference', 'pca', 'object detection', 'knitr', 'matplotlib', 'word embedding', 'teradata ']\n",
      "['factor analysis', 'decision trees', 'yolo', '', 'scipy', 'lsi', 'google big query']\n",
      "['forecasting', 'svd', 'gpu', '', 'bokeh', 'spacy', 'aws lambda']\n",
      "['markov chain', 'ensemble models', 'cuda', '', 'statsmodel', 'gensim', 'aws emr']\n",
      "['monte carlo', 'boltzman machine', 'tensorflow', '', '', 'nltk', 'hive']\n",
      "['', '', 'lstm', '', '', 'nmf', 'hadoop']\n",
      "['', '', 'gan', '', '', 'doc2vec', 'sql']\n",
      "['', '', 'opencv', '', '', 'cbow', '']\n",
      "['', '', '', '', '', 'bag of words', '']\n",
      "['', '', '', '', '', 'skip gram', '']\n",
      "['', '', '', '', '', 'bert', '']\n",
      "['', '', '', '', '', 'sentiment analysis', '']\n",
      "['', '', '', '', '', 'chat bot', '']\n"
     ]
    }
   ],
   "source": [
    "#for row in a:\n",
    "#    print (row)\n",
    "    \n",
    "    \n",
    "with open('template.csv','rt')as f:\n",
    "    data = csv.reader(f)\n",
    "    for row in data:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Statistics     Machine Learning  \\\n",
      "0     statistical models    linear regression   \n",
      "1   statistical modeling  logistic regression   \n",
      "2            probability              K means   \n",
      "3    normal distribution       random forest    \n",
      "4   poisson distribution              xgboost   \n",
      "5        survival models                  svm   \n",
      "6     hypothesis testing          naïve bayes   \n",
      "7     bayesian inference                  pca   \n",
      "8        factor analysis       decision trees   \n",
      "9            forecasting                  svd   \n",
      "10          markov chain      ensemble models   \n",
      "11           monte carlo     boltzman machine   \n",
      "12                   NaN                  NaN   \n",
      "13                   NaN                  NaN   \n",
      "14                   NaN                  NaN   \n",
      "15                   NaN                  NaN   \n",
      "16                   NaN                  NaN   \n",
      "17                   NaN                  NaN   \n",
      "18                   NaN                  NaN   \n",
      "19                   NaN                  NaN   \n",
      "\n",
      "                         Deep Learning R Language Python Language  \\\n",
      "0                       neural network          r          python   \n",
      "1                                keras     ggplot           flask   \n",
      "2                               theano      shiny          django   \n",
      "3                      face detection        cran          pandas   \n",
      "4                      neural networks      dplyr           numpy   \n",
      "5   convolutional neural network (cnn)      tidyr     scikitlearn   \n",
      "6        recurrent neural network(RNN)  lubridate        sklearn    \n",
      "7                     object detection      knitr      matplotlib   \n",
      "8                                 yolo        NaN           scipy   \n",
      "9                                  gpu        NaN           bokeh   \n",
      "10                                cuda        NaN      statsmodel   \n",
      "11                          tensorflow        NaN             NaN   \n",
      "12                                lstm        NaN             NaN   \n",
      "13                                 gan        NaN             NaN   \n",
      "14                              opencv        NaN             NaN   \n",
      "15                                 NaN        NaN             NaN   \n",
      "16                                 NaN        NaN             NaN   \n",
      "17                                 NaN        NaN             NaN   \n",
      "18                                 NaN        NaN             NaN   \n",
      "19                                 NaN        NaN             NaN   \n",
      "\n",
      "                            NLP  Data Engineering  \n",
      "0                           nlp               aws  \n",
      "1   natural language processing               ec2  \n",
      "2                topic modeling   amazon redshift  \n",
      "3                           lda                s3  \n",
      "4      named entity recognition            docker  \n",
      "5                   pos tagging        kubernetes  \n",
      "6                      word2vec             scala  \n",
      "7                word embedding         teradata   \n",
      "8                           lsi  google big query  \n",
      "9                         spacy        aws lambda  \n",
      "10                       gensim           aws emr  \n",
      "11                         nltk              hive  \n",
      "12                          nmf            hadoop  \n",
      "13                      doc2vec               sql  \n",
      "14                         cbow               NaN  \n",
      "15                 bag of words               NaN  \n",
      "16                    skip gram               NaN  \n",
      "17                         bert               NaN  \n",
      "18           sentiment analysis               NaN  \n",
      "19                     chat bot               NaN  \n"
     ]
    }
   ],
   "source": [
    "keyword_dict=pd.read_csv('template.csv')\n",
    "print(keyword_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that does phrase matching and builds a candidate profile\n",
    "def create_profile(file):\n",
    "    text = pdfextract(file) \n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.lower()\n",
    "    #below is the csv where we have all the keywords, you can customize your own\n",
    "    keyword_dict = pd.read_csv('D:/NLP_Resume/resume/template_new.csv')\n",
    "    stats_words = [nlp(text) for text in keyword_dict['Statistics'].dropna(axis = 0)]\n",
    "    NLP_words = [nlp(text) for text in keyword_dict['NLP'].dropna(axis = 0)]\n",
    "    ML_words = [nlp(text) for text in keyword_dict['Machine Learning'].dropna(axis = 0)]\n",
    "    DL_words = [nlp(text) for text in keyword_dict['Deep Learning'].dropna(axis = 0)]\n",
    "    R_words = [nlp(text) for text in keyword_dict['R Language'].dropna(axis = 0)]\n",
    "    python_words = [nlp(text) for text in keyword_dict['Python Language'].dropna(axis = 0)]\n",
    "    Data_Engineering_words = [nlp(text) for text in keyword_dict['Data Engineering'].dropna(axis = 0)]\n",
    "\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add('Stats', None, *stats_words)\n",
    "    matcher.add('NLP', None, *NLP_words)\n",
    "    matcher.add('ML', None, *ML_words)\n",
    "    matcher.add('DL', None, *DL_words)\n",
    "    matcher.add('R', None, *R_words)\n",
    "    matcher.add('Python', None, *python_words)\n",
    "    matcher.add('DE', None, *Data_Engineering_words)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    d = []  \n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "        span = doc[start : end]  # get the matched slice of the doc\n",
    "        d.append((rule_id, span.text))      \n",
    "    keywords = \"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function that does phrase matching and builds a candidate profile\n",
    "def create_profile(file):\n",
    "    text = pdfextract(file) \n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.lower()\n",
    "    #below is the csv where we have all the keywords, you can customize your own\n",
    "    keyword_dict = pd.read_csv('D:/NLP_Resume/resume/template_new.csv')\n",
    "    stats_words = [nlp(text) for text in keyword_dict['Statistics'].dropna(axis = 0)]\n",
    "    NLP_words = [nlp(text) for text in keyword_dict['NLP'].dropna(axis = 0)]\n",
    "    ML_words = [nlp(text) for text in keyword_dict['Machine Learning'].dropna(axis = 0)]\n",
    "    DL_words = [nlp(text) for text in keyword_dict['Deep Learning'].dropna(axis = 0)]\n",
    "    R_words = [nlp(text) for text in keyword_dict['R Language'].dropna(axis = 0)]\n",
    "    python_words = [nlp(text) for text in keyword_dict['Python Language'].dropna(axis = 0)]\n",
    "    Data_Engineering_words = [nlp(text) for text in keyword_dict['Data Engineering'].dropna(axis = 0)]\n",
    "\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add('Stats', None, *stats_words)\n",
    "    matcher.add('NLP', None, *NLP_words)\n",
    "    matcher.add('ML', None, *ML_words)\n",
    "    matcher.add('DL', None, *DL_words)\n",
    "    matcher.add('R', None, *R_words)\n",
    "    matcher.add('Python', None, *python_words)\n",
    "    matcher.add('DE', None, *Data_Engineering_words)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    d = []  \n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "        span = doc[start : end]  # get the matched slice of the doc\n",
    "        d.append((rule_id, span.text))      \n",
    "    keywords = \"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())\n",
    "    \n",
    "    ## convertimg string of keywords to dataframe\n",
    "    df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])\n",
    "    df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])\n",
    "    df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])\n",
    "    df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) \n",
    "    df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(\")\"))\n",
    "    \n",
    "    base = os.path.basename(file)\n",
    "    filename = os.path.splitext(base)[0]\n",
    "       \n",
    "    name = filename.split('_')\n",
    "    name2 = name[0]\n",
    "    name2 = name2.lower()\n",
    "    ## converting str to dataframe\n",
    "    name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])\n",
    "    \n",
    "    dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)\n",
    "    dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)\n",
    "\n",
    "    return(dataf)\n",
    "        \n",
    "#function ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "PdfReadError",
     "evalue": "Could not read malformed PDF file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPdfReadError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-185bdc851e4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monlyfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monlyfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mfinal_database\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fda2eb67e753>\u001b[0m in \u001b[0;36mcreate_profile\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#function that does phrase matching and builds a candidate profile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdfextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-24bc6736cf4f>\u001b[0m in \u001b[0;36mpdfextract\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpdfextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfileReader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#it reads three files one by one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcountpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m#number of pages in a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PyPDF2/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, strict, warndest, overwriteWarnings)\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PyPDF2/pdf.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlast1K\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EOF marker not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNextEndLine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  line:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PyPDF2/pdf.py\u001b[0m in \u001b[0;36mreadNextEndLine\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m   1935\u001b[0m             \u001b[0;31m# Prevent infinite loops in malformed PDFs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1937\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not read malformed PDF file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1938\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  x:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%x\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPdfReadError\u001b[0m: Could not read malformed PDF file"
     ]
    }
   ],
   "source": [
    "#code to execute/call the above functions\n",
    "\n",
    "final_database=pd.DataFrame()\n",
    "i = 0 \n",
    "while i < len(onlyfiles):\n",
    "    file = onlyfiles[i]\n",
    "    dat = create_profile(file)\n",
    "    final_database = final_database.append(dat)\n",
    "    i +=1\n",
    "    print(final_database)\n",
    "\n",
    "    #code to count words under each category and visulaize it through Matplotlib\n",
    "\n",
    "final_database2 = final_database['Keyword'].groupby([final_database['Candidate Name'], final_database['Subject']]).count().unstack()\n",
    "final_database2.reset_index(inplace = True)\n",
    "final_database2.fillna(0,inplace=True)\n",
    "new_data = final_database2.iloc[:,1:]\n",
    "new_data.index = final_database2['Candidate Name']\n",
    "#execute the below line if you want to see the candidate profile in a csv format\n",
    "#sample2=new_data.to_csv('sample.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "ax = new_data.plot.barh(title=\"Resume keywords by category\", legend=False, figsize=(25,7), stacked=True)\n",
    "labels = []\n",
    "for j in new_data.columns:\n",
    "    for i in new_data.index:\n",
    "        label = str(j)+\": \" + str(new_data.loc[i][j])\n",
    "        labels.append(label)\n",
    "patches = ax.patches\n",
    "for label, rect in zip(labels, patches):\n",
    "    width = rect.get_width()\n",
    "    if width > 0:\n",
    "        x = rect.get_x()\n",
    "        y = rect.get_y()\n",
    "        height = rect.get_height()\n",
    "        ax.text(x + width/2., y + height/2., label, ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
